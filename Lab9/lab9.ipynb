{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13fa8e6c4af1469f95493c7463aac50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46968a209fc7410b8483addf735e240f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_17d64a49177d4e80beb40e1d0a9a3340",
              "IPY_MODEL_95a44c45df784d5dbbefb8bda8ff37d8"
            ]
          }
        },
        "46968a209fc7410b8483addf735e240f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17d64a49177d4e80beb40e1d0a9a3340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aed89f359e694a209bee4014a66c105f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69acdd7e462240c588cebad00ae7a3e2"
          }
        },
        "95a44c45df784d5dbbefb8bda8ff37d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9073f715d4dc4ca79082e4653a0cf058",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230M/230M [00:01&lt;00:00, 219MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e014614e20a4f61a32c559915bf085e"
          }
        },
        "aed89f359e694a209bee4014a66c105f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69acdd7e462240c588cebad00ae7a3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9073f715d4dc4ca79082e4653a0cf058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e014614e20a4f61a32c559915bf085e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c784d2bb33804f6b8bcd7119d1c6bd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5005e7b651f74dba97fb8515066be2fc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8153ec76ee7144c58f2f1a2dcc812a37",
              "IPY_MODEL_9d53355c481a471a84ccaf898e6439d4"
            ]
          }
        },
        "5005e7b651f74dba97fb8515066be2fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8153ec76ee7144c58f2f1a2dcc812a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8bf49ed3b99641468fc8468ee0e42e2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31b9479c252c4a3ba6bb044dc53d4b12"
          }
        },
        "9d53355c481a471a84ccaf898e6439d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc9f548958d0435280fb77ab213c7359",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "1466368it [00:03, 464558.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cebe2bc16044c1a95de5c76bcac62f8"
          }
        },
        "8bf49ed3b99641468fc8468ee0e42e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31b9479c252c4a3ba6bb044dc53d4b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc9f548958d0435280fb77ab213c7359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cebe2bc16044c1a95de5c76bcac62f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rEwpv0GZ_CrT"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xnoEhAVvBcMj"
      },
      "source": [
        "#Lab 10: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sBOvJdJfkXIL"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GiuvTUWOjtBC"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dKzRORuLBNLR",
        "colab": {}
      },
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pdb\n",
        "import gc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S4R3D8Mr8b54"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFoEeTYHDq2s"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7z6g7a_Y84n0"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P8NtFZRd5hcm"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/akash2907/bird-species-classification\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TBigIUFTukeJ"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_X29UC6CvwfQ"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mhjc0pM7jOoZ",
        "outputId": "0d0cc342-97e6-4f85-c4e3-76ea148d1f3c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d4903ae-db83-4ba7-86fe-1d8dd1f9f5ee\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2d4903ae-db83-4ba7-86fe-1d8dd1f9f5ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 67 Mar 11 23:51 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AGlIa4SIwEXB"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7HtB-XdIr1EE",
        "outputId": "d33ac96d-89c3-459b-e5d0-b7bc8fbc9aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 99% 342M/346M [00:07<00:00, 69.3MB/s]\n",
            "100% 346M/346M [00:07<00:00, 48.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zcz0JGXjxFGe"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and make your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5jHFdToeDtIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c1053e6e-7bc6-4898-ea9f-32bc5c3e4a08"
      },
      "source": [
        "class IntelDataset(Dataset):\n",
        "    def __init__(self, zip_file='./intel-image-classification.zip', size=256, train=True, upload=False):\n",
        "        super(IntelDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_train', 'seg_train'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_test', 'seg_test'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.dataset_folder)\n",
        "        return 100\n",
        "\n",
        "intel_data = IntelDataset()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./intel-image-classification.zip\n",
            "Resizing images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2vJVbYcAJAf2"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gMOzGDND9FD1"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jLvmDHbl9IyG"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eOtl8z8G9wbr"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AY-XU4Mwas0j",
        "colab": {}
      },
      "source": [
        "class ResNetIntel(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetIntel, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "\n",
        "        self.model = resnet152(pretrained=True)\n",
        "        \n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "          self.model.requires_grad=False\n",
        "            # Turn off all gradients of the resnet\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        myFC = nn.Linear(2048, num_classes)\n",
        "        self.model.fc = myFC\n",
        "        self.model.fc.requires_grad = True\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        # length = len(self.model.modules)\n",
        "        # list(self.model.modules())[-n_layers:].requires_grad = True\n",
        "        start_index = 152 - n_layers\n",
        "        i = 0\n",
        "        for param in self.model.parameters():\n",
        "          if(i > start_index):\n",
        "            param.requires_grad = True\n",
        "\n",
        "          i += 1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Krh0eYy18R9"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOGrrw2gbIPf",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VKESMcKi2E_f",
        "colab": {}
      },
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 10\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 5\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    train_dataset = IntelDataset(upload=True, train=True)\n",
        "    val_dataset = IntelDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = ResNetIntel(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            model.unfreeze(epoch)\n",
        "            \n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            # pdb.set_trace()\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fvnxeLotchiH",
        "outputId": "77543739-9954-4bf8-db40-905242630750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "13fa8e6c4af1469f95493c7463aac50f",
            "46968a209fc7410b8483addf735e240f",
            "17d64a49177d4e80beb40e1d0a9a3340",
            "95a44c45df784d5dbbefb8bda8ff37d8",
            "aed89f359e694a209bee4014a66c105f",
            "69acdd7e462240c588cebad00ae7a3e2",
            "9073f715d4dc4ca79082e4653a0cf058",
            "2e014614e20a4f61a32c559915bf085e"
          ]
        }
      },
      "source": [
        "gc.collect() \n",
        "train(start_frozen=False, model_unfreeze=0)  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13fa8e6c4af1469f95493c7463aac50f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=241530880), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss:0.0191, train accuracy:1.0000.: 100%|██████████| 200/200 [02:35<00:00,  1.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuSulVATS5K6UUhHo/wMarFZMou1d2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aEDv_-H7BvM0"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YH5mQBaa-_0b"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u_YmE1pe-6LF"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XMKRI77_-8nc"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KaUc8BTYC1bz"
      },
      "source": [
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch).\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qBT5jgifC7Im"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mg9ySEO_BNDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bda5a7e-f526-4ae0-9303-41b439a4cdfa"
      },
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "############################\n",
        "gc.collect()\n",
        "train(start_frozen=True, model_unfreeze=5)  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:3.1283, train accuracy:0.0000.:   0%|          | 0/200 [00:02<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:3.1283, train accuracy:0.0000.:   0%|          | 1/200 [00:02<08:09,  2.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:3.0211, train accuracy:0.0000.:   0%|          | 1/200 [00:02<08:09,  2.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:3.0211, train accuracy:0.0000.:   1%|          | 2/200 [00:02<05:54,  1.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.9359, train accuracy:0.0000.:   1%|          | 2/200 [00:02<05:54,  1.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.9359, train accuracy:0.0000.:   2%|▏         | 3/200 [00:02<04:20,  1.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.8193, train accuracy:0.0000.:   2%|▏         | 3/200 [00:04<04:20,  1.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.8193, train accuracy:0.0000.:   2%|▏         | 4/200 [00:04<04:52,  1.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.7351, train accuracy:0.0000.:   2%|▏         | 4/200 [00:05<04:52,  1.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.7351, train accuracy:0.0000.:   2%|▎         | 5/200 [00:05<03:36,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.6302, train accuracy:0.2000.:   2%|▎         | 5/200 [00:05<03:36,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.6302, train accuracy:0.2000.:   3%|▎         | 6/200 [00:05<02:43,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.5463, train accuracy:0.2000.:   3%|▎         | 6/200 [00:07<02:43,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.5463, train accuracy:0.2000.:   4%|▎         | 7/200 [00:07<03:41,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.4466, train accuracy:0.2000.:   4%|▎         | 7/200 [00:07<03:41,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.4466, train accuracy:0.2000.:   4%|▍         | 8/200 [00:07<02:46,  1.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.3470, train accuracy:0.4000.:   4%|▍         | 8/200 [00:07<02:46,  1.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.3470, train accuracy:0.4000.:   4%|▍         | 9/200 [00:07<02:07,  1.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.2495, train accuracy:0.2000.:   4%|▍         | 9/200 [00:09<02:07,  1.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.2495, train accuracy:0.2000.:   5%|▌         | 10/200 [00:09<03:15,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.1619, train accuracy:0.6000.:   5%|▌         | 10/200 [00:09<03:15,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.1619, train accuracy:0.6000.:   6%|▌         | 11/200 [00:09<02:27,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.0694, train accuracy:0.8000.:   6%|▌         | 11/200 [00:09<02:27,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:2.0694, train accuracy:0.8000.:   6%|▌         | 12/200 [00:09<01:53,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.9897, train accuracy:0.8000.:   6%|▌         | 12/200 [00:11<01:53,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.9897, train accuracy:0.8000.:   6%|▋         | 13/200 [00:11<03:03,  1.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.8790, train accuracy:1.0000.:   6%|▋         | 13/200 [00:11<03:03,  1.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.8790, train accuracy:1.0000.:   7%|▋         | 14/200 [00:11<02:20,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.8143, train accuracy:1.0000.:   7%|▋         | 14/200 [00:12<02:20,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.8143, train accuracy:1.0000.:   8%|▊         | 15/200 [00:12<01:49,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.6898, train accuracy:1.0000.:   8%|▊         | 15/200 [00:13<01:49,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.6898, train accuracy:1.0000.:   8%|▊         | 16/200 [00:13<02:58,  1.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.6213, train accuracy:1.0000.:   8%|▊         | 16/200 [00:14<02:58,  1.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.6213, train accuracy:1.0000.:   8%|▊         | 17/200 [00:14<02:16,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.5175, train accuracy:1.0000.:   8%|▊         | 17/200 [00:14<02:16,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.5175, train accuracy:1.0000.:   9%|▉         | 18/200 [00:14<01:46,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.4957, train accuracy:1.0000.:   9%|▉         | 18/200 [00:16<01:46,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.4957, train accuracy:1.0000.:  10%|▉         | 19/200 [00:16<02:56,  1.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.3623, train accuracy:1.0000.:  10%|▉         | 19/200 [00:16<02:56,  1.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.3623, train accuracy:1.0000.:  10%|█         | 20/200 [00:16<02:13,  1.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.2809, train accuracy:1.0000.:  10%|█         | 20/200 [00:17<02:13,  1.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.2809, train accuracy:1.0000.:  10%|█         | 21/200 [00:17<02:20,  1.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.2077, train accuracy:1.0000.:  10%|█         | 21/200 [00:19<02:20,  1.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.2077, train accuracy:1.0000.:  11%|█         | 22/200 [00:19<03:18,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.1560, train accuracy:1.0000.:  11%|█         | 22/200 [00:19<03:18,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.1560, train accuracy:1.0000.:  12%|█▏        | 23/200 [00:19<02:30,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.0669, train accuracy:1.0000.:  12%|█▏        | 23/200 [00:19<02:30,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:1.0669, train accuracy:1.0000.:  12%|█▏        | 24/200 [00:19<01:55,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.9867, train accuracy:1.0000.:  12%|█▏        | 24/200 [00:21<01:55,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.9867, train accuracy:1.0000.:  12%|█▎        | 25/200 [00:21<02:59,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.9178, train accuracy:1.0000.:  12%|█▎        | 25/200 [00:21<02:59,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.9178, train accuracy:1.0000.:  13%|█▎        | 26/200 [00:21<02:15,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.8696, train accuracy:1.0000.:  13%|█▎        | 26/200 [00:21<02:15,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.8696, train accuracy:1.0000.:  14%|█▎        | 27/200 [00:21<01:45,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.8034, train accuracy:1.0000.:  14%|█▎        | 27/200 [00:23<01:45,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.8034, train accuracy:1.0000.:  14%|█▍        | 28/200 [00:23<02:52,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.7402, train accuracy:1.0000.:  14%|█▍        | 28/200 [00:24<02:52,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.7402, train accuracy:1.0000.:  14%|█▍        | 29/200 [00:24<02:10,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.7278, train accuracy:1.0000.:  14%|█▍        | 29/200 [00:24<02:10,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.7278, train accuracy:1.0000.:  15%|█▌        | 30/200 [00:24<01:41,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.6475, train accuracy:1.0000.:  15%|█▌        | 30/200 [00:26<01:41,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.6475, train accuracy:1.0000.:  16%|█▌        | 31/200 [00:26<02:46,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.6118, train accuracy:1.0000.:  16%|█▌        | 31/200 [00:26<02:46,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.6118, train accuracy:1.0000.:  16%|█▌        | 32/200 [00:26<02:07,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5971, train accuracy:1.0000.:  16%|█▌        | 32/200 [00:26<02:07,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5971, train accuracy:1.0000.:  16%|█▋        | 33/200 [00:26<01:38,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5532, train accuracy:1.0000.:  16%|█▋        | 33/200 [00:28<01:38,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5532, train accuracy:1.0000.:  17%|█▋        | 34/200 [00:28<02:43,  1.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5124, train accuracy:1.0000.:  17%|█▋        | 34/200 [00:28<02:43,  1.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.5124, train accuracy:1.0000.:  18%|█▊        | 35/200 [00:28<02:04,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.4464, train accuracy:1.0000.:  18%|█▊        | 35/200 [00:28<02:04,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.4464, train accuracy:1.0000.:  18%|█▊        | 36/200 [00:28<01:36,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.4471, train accuracy:1.0000.:  18%|█▊        | 36/200 [00:30<01:36,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.4471, train accuracy:1.0000.:  18%|█▊        | 37/200 [00:30<02:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3872, train accuracy:1.0000.:  18%|█▊        | 37/200 [00:31<02:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3872, train accuracy:1.0000.:  19%|█▉        | 38/200 [00:31<02:04,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3695, train accuracy:1.0000.:  19%|█▉        | 38/200 [00:31<02:04,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3695, train accuracy:1.0000.:  20%|█▉        | 39/200 [00:31<01:38,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3840, train accuracy:1.0000.:  20%|█▉        | 39/200 [00:33<01:38,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3840, train accuracy:1.0000.:  20%|██        | 40/200 [00:33<02:38,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3139, train accuracy:1.0000.:  20%|██        | 40/200 [00:34<02:38,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.3139, train accuracy:1.0000.:  20%|██        | 41/200 [00:34<02:31,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2987, train accuracy:1.0000.:  20%|██        | 41/200 [00:34<02:31,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2987, train accuracy:1.0000.:  21%|██        | 42/200 [00:34<01:56,  1.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2706, train accuracy:1.0000.:  21%|██        | 42/200 [00:36<01:56,  1.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2706, train accuracy:1.0000.:  22%|██▏       | 43/200 [00:36<02:52,  1.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2613, train accuracy:1.0000.:  22%|██▏       | 43/200 [00:36<02:52,  1.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2613, train accuracy:1.0000.:  22%|██▏       | 44/200 [00:36<02:11,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2564, train accuracy:1.0000.:  22%|██▏       | 44/200 [00:36<02:11,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2564, train accuracy:1.0000.:  22%|██▎       | 45/200 [00:36<01:42,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2463, train accuracy:1.0000.:  22%|██▎       | 45/200 [00:38<01:42,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2463, train accuracy:1.0000.:  23%|██▎       | 46/200 [00:38<02:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2076, train accuracy:1.0000.:  23%|██▎       | 46/200 [00:38<02:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2076, train accuracy:1.0000.:  24%|██▎       | 47/200 [00:38<02:01,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2290, train accuracy:1.0000.:  24%|██▎       | 47/200 [00:39<02:01,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2290, train accuracy:1.0000.:  24%|██▍       | 48/200 [00:39<01:34,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2050, train accuracy:1.0000.:  24%|██▍       | 48/200 [00:41<01:34,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.2050, train accuracy:1.0000.:  24%|██▍       | 49/200 [00:41<02:34,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1792, train accuracy:1.0000.:  24%|██▍       | 49/200 [00:41<02:34,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1792, train accuracy:1.0000.:  25%|██▌       | 50/200 [00:41<01:57,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1834, train accuracy:1.0000.:  25%|██▌       | 50/200 [00:41<01:57,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1834, train accuracy:1.0000.:  26%|██▌       | 51/200 [00:41<01:31,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1597, train accuracy:1.0000.:  26%|██▌       | 51/200 [00:43<01:31,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1597, train accuracy:1.0000.:  26%|██▌       | 52/200 [00:43<02:28,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1778, train accuracy:1.0000.:  26%|██▌       | 52/200 [00:43<02:28,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1778, train accuracy:1.0000.:  26%|██▋       | 53/200 [00:43<01:53,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1499, train accuracy:1.0000.:  26%|██▋       | 53/200 [00:43<01:53,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1499, train accuracy:1.0000.:  27%|██▋       | 54/200 [00:43<01:28,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1705, train accuracy:1.0000.:  27%|██▋       | 54/200 [00:45<01:28,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1705, train accuracy:1.0000.:  28%|██▊       | 55/200 [00:45<02:25,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1490, train accuracy:1.0000.:  28%|██▊       | 55/200 [00:46<02:25,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1490, train accuracy:1.0000.:  28%|██▊       | 56/200 [00:46<01:50,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1366, train accuracy:1.0000.:  28%|██▊       | 56/200 [00:46<01:50,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1366, train accuracy:1.0000.:  28%|██▊       | 57/200 [00:46<01:25,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1442, train accuracy:1.0000.:  28%|██▊       | 57/200 [00:48<01:25,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1442, train accuracy:1.0000.:  29%|██▉       | 58/200 [00:48<02:20,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1171, train accuracy:1.0000.:  29%|██▉       | 58/200 [00:48<02:20,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1171, train accuracy:1.0000.:  30%|██▉       | 59/200 [00:48<01:46,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1161, train accuracy:1.0000.:  30%|██▉       | 59/200 [00:48<01:46,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1161, train accuracy:1.0000.:  30%|███       | 60/200 [00:48<01:23,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1114, train accuracy:1.0000.:  30%|███       | 60/200 [00:51<01:23,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1114, train accuracy:1.0000.:  30%|███       | 61/200 [00:51<02:47,  1.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1162, train accuracy:1.0000.:  30%|███       | 61/200 [00:51<02:47,  1.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1162, train accuracy:1.0000.:  31%|███       | 62/200 [00:51<02:05,  1.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1045, train accuracy:1.0000.:  31%|███       | 62/200 [00:51<02:05,  1.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1045, train accuracy:1.0000.:  32%|███▏      | 63/200 [00:51<01:36,  1.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1011, train accuracy:1.0000.:  32%|███▏      | 63/200 [00:53<01:36,  1.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1011, train accuracy:1.0000.:  32%|███▏      | 64/200 [00:53<02:26,  1.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1002, train accuracy:1.0000.:  32%|███▏      | 64/200 [00:53<02:26,  1.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1002, train accuracy:1.0000.:  32%|███▎      | 65/200 [00:53<01:50,  1.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0980, train accuracy:1.0000.:  32%|███▎      | 65/200 [00:54<01:50,  1.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0980, train accuracy:1.0000.:  33%|███▎      | 66/200 [00:54<01:26,  1.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1045, train accuracy:1.0000.:  33%|███▎      | 66/200 [00:55<01:26,  1.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1045, train accuracy:1.0000.:  34%|███▎      | 67/200 [00:56<02:17,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1119, train accuracy:1.0000.:  34%|███▎      | 67/200 [00:56<02:17,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1119, train accuracy:1.0000.:  34%|███▍      | 68/200 [00:56<01:44,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0964, train accuracy:1.0000.:  34%|███▍      | 68/200 [00:56<01:44,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0964, train accuracy:1.0000.:  34%|███▍      | 69/200 [00:56<01:21,  1.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0911, train accuracy:1.0000.:  34%|███▍      | 69/200 [00:58<01:21,  1.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0911, train accuracy:1.0000.:  35%|███▌      | 70/200 [00:58<02:12,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1029, train accuracy:1.0000.:  35%|███▌      | 70/200 [00:58<02:12,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.1029, train accuracy:1.0000.:  36%|███▌      | 71/200 [00:58<01:40,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0888, train accuracy:1.0000.:  36%|███▌      | 71/200 [00:58<01:40,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0888, train accuracy:1.0000.:  36%|███▌      | 72/200 [00:58<01:17,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0845, train accuracy:1.0000.:  36%|███▌      | 72/200 [01:00<01:17,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0845, train accuracy:1.0000.:  36%|███▋      | 73/200 [01:00<02:08,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0813, train accuracy:1.0000.:  36%|███▋      | 73/200 [01:01<02:08,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0813, train accuracy:1.0000.:  37%|███▋      | 74/200 [01:01<01:38,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0851, train accuracy:1.0000.:  37%|███▋      | 74/200 [01:01<01:38,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0851, train accuracy:1.0000.:  38%|███▊      | 75/200 [01:01<01:16,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0963, train accuracy:1.0000.:  38%|███▊      | 75/200 [01:03<01:16,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0963, train accuracy:1.0000.:  38%|███▊      | 76/200 [01:03<02:06,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0780, train accuracy:1.0000.:  38%|███▊      | 76/200 [01:03<02:06,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0780, train accuracy:1.0000.:  38%|███▊      | 77/200 [01:03<01:36,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0870, train accuracy:1.0000.:  38%|███▊      | 77/200 [01:03<01:36,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0870, train accuracy:1.0000.:  39%|███▉      | 78/200 [01:03<01:15,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0816, train accuracy:1.0000.:  39%|███▉      | 78/200 [01:05<01:15,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0816, train accuracy:1.0000.:  40%|███▉      | 79/200 [01:05<02:02,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0780, train accuracy:1.0000.:  40%|███▉      | 79/200 [01:05<02:02,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0780, train accuracy:1.0000.:  40%|████      | 80/200 [01:05<01:32,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0782, train accuracy:1.0000.:  40%|████      | 80/200 [01:06<01:32,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0782, train accuracy:1.0000.:  40%|████      | 81/200 [01:06<01:34,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0629, train accuracy:1.0000.:  40%|████      | 81/200 [01:08<01:34,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0629, train accuracy:1.0000.:  41%|████      | 82/200 [01:08<02:15,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0762, train accuracy:1.0000.:  41%|████      | 82/200 [01:08<02:15,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0762, train accuracy:1.0000.:  42%|████▏     | 83/200 [01:08<01:42,  1.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0755, train accuracy:1.0000.:  42%|████▏     | 83/200 [01:09<01:42,  1.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0755, train accuracy:1.0000.:  42%|████▏     | 84/200 [01:09<01:19,  1.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0797, train accuracy:1.0000.:  42%|████▏     | 84/200 [01:11<01:19,  1.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0797, train accuracy:1.0000.:  42%|████▎     | 85/200 [01:11<02:01,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0627, train accuracy:1.0000.:  42%|████▎     | 85/200 [01:11<02:01,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0627, train accuracy:1.0000.:  43%|████▎     | 86/200 [01:11<01:31,  1.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0732, train accuracy:1.0000.:  43%|████▎     | 86/200 [01:11<01:31,  1.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0732, train accuracy:1.0000.:  44%|████▎     | 87/200 [01:11<01:10,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0688, train accuracy:1.0000.:  44%|████▎     | 87/200 [01:13<01:10,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0688, train accuracy:1.0000.:  44%|████▍     | 88/200 [01:13<01:53,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0652, train accuracy:1.0000.:  44%|████▍     | 88/200 [01:13<01:53,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0652, train accuracy:1.0000.:  44%|████▍     | 89/200 [01:13<01:26,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0773, train accuracy:1.0000.:  44%|████▍     | 89/200 [01:13<01:26,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0773, train accuracy:1.0000.:  45%|████▌     | 90/200 [01:13<01:06,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0679, train accuracy:1.0000.:  45%|████▌     | 90/200 [01:15<01:06,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0679, train accuracy:1.0000.:  46%|████▌     | 91/200 [01:15<01:50,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0641, train accuracy:1.0000.:  46%|████▌     | 91/200 [01:16<01:50,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0641, train accuracy:1.0000.:  46%|████▌     | 92/200 [01:16<01:24,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0677, train accuracy:1.0000.:  46%|████▌     | 92/200 [01:16<01:24,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0677, train accuracy:1.0000.:  46%|████▋     | 93/200 [01:16<01:05,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0645, train accuracy:1.0000.:  46%|████▋     | 93/200 [01:18<01:05,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0645, train accuracy:1.0000.:  47%|████▋     | 94/200 [01:18<01:46,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0613, train accuracy:1.0000.:  47%|████▋     | 94/200 [01:18<01:46,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0613, train accuracy:1.0000.:  48%|████▊     | 95/200 [01:18<01:20,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0607, train accuracy:1.0000.:  48%|████▊     | 95/200 [01:18<01:20,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0607, train accuracy:1.0000.:  48%|████▊     | 96/200 [01:18<01:02,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0717, train accuracy:1.0000.:  48%|████▊     | 96/200 [01:20<01:02,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0717, train accuracy:1.0000.:  48%|████▊     | 97/200 [01:20<01:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0634, train accuracy:1.0000.:  48%|████▊     | 97/200 [01:20<01:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0634, train accuracy:1.0000.:  49%|████▉     | 98/200 [01:20<01:17,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0544, train accuracy:1.0000.:  49%|████▉     | 98/200 [01:20<01:17,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0544, train accuracy:1.0000.:  50%|████▉     | 99/200 [01:20<01:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0517, train accuracy:1.0000.:  50%|████▉     | 99/200 [01:22<01:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0517, train accuracy:1.0000.:  50%|█████     | 100/200 [01:22<01:39,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0536, train accuracy:1.0000.:  50%|█████     | 100/200 [01:23<01:39,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0536, train accuracy:1.0000.:  50%|█████     | 101/200 [01:23<01:35,  1.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0568, train accuracy:1.0000.:  50%|█████     | 101/200 [01:23<01:35,  1.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0568, train accuracy:1.0000.:  51%|█████     | 102/200 [01:23<01:12,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0576, train accuracy:1.0000.:  51%|█████     | 102/200 [01:25<01:12,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0576, train accuracy:1.0000.:  52%|█████▏    | 103/200 [01:25<01:47,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0560, train accuracy:1.0000.:  52%|█████▏    | 103/200 [01:26<01:47,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0560, train accuracy:1.0000.:  52%|█████▏    | 104/200 [01:26<01:21,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0505, train accuracy:1.0000.:  52%|█████▏    | 104/200 [01:26<01:21,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0505, train accuracy:1.0000.:  52%|█████▎    | 105/200 [01:26<01:02,  1.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0596, train accuracy:1.0000.:  52%|█████▎    | 105/200 [01:28<01:02,  1.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0596, train accuracy:1.0000.:  53%|█████▎    | 106/200 [01:28<01:37,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0527, train accuracy:1.0000.:  53%|█████▎    | 106/200 [01:28<01:37,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0527, train accuracy:1.0000.:  54%|█████▎    | 107/200 [01:28<01:13,  1.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0628, train accuracy:1.0000.:  54%|█████▎    | 107/200 [01:28<01:13,  1.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0628, train accuracy:1.0000.:  54%|█████▍    | 108/200 [01:28<00:56,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0511, train accuracy:1.0000.:  54%|█████▍    | 108/200 [01:30<00:56,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0511, train accuracy:1.0000.:  55%|█████▍    | 109/200 [01:30<01:31,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0505, train accuracy:1.0000.:  55%|█████▍    | 109/200 [01:30<01:31,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0505, train accuracy:1.0000.:  55%|█████▌    | 110/200 [01:30<01:09,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0514, train accuracy:1.0000.:  55%|█████▌    | 110/200 [01:31<01:09,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0514, train accuracy:1.0000.:  56%|█████▌    | 111/200 [01:31<00:54,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0475, train accuracy:1.0000.:  56%|█████▌    | 111/200 [01:33<00:54,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0475, train accuracy:1.0000.:  56%|█████▌    | 112/200 [01:33<01:28,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0474, train accuracy:1.0000.:  56%|█████▌    | 112/200 [01:33<01:28,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0474, train accuracy:1.0000.:  56%|█████▋    | 113/200 [01:33<01:07,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0455, train accuracy:1.0000.:  56%|█████▋    | 113/200 [01:33<01:07,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0455, train accuracy:1.0000.:  57%|█████▋    | 114/200 [01:33<00:51,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0450, train accuracy:1.0000.:  57%|█████▋    | 114/200 [01:35<00:51,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0450, train accuracy:1.0000.:  57%|█████▊    | 115/200 [01:35<01:25,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0496, train accuracy:1.0000.:  57%|█████▊    | 115/200 [01:35<01:25,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0496, train accuracy:1.0000.:  58%|█████▊    | 116/200 [01:35<01:04,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0593, train accuracy:1.0000.:  58%|█████▊    | 116/200 [01:35<01:04,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0593, train accuracy:1.0000.:  58%|█████▊    | 117/200 [01:35<00:49,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0567, train accuracy:1.0000.:  58%|█████▊    | 117/200 [01:37<00:49,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0567, train accuracy:1.0000.:  59%|█████▉    | 118/200 [01:37<01:21,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0545, train accuracy:1.0000.:  59%|█████▉    | 118/200 [01:37<01:21,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0545, train accuracy:1.0000.:  60%|█████▉    | 119/200 [01:37<01:01,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0503, train accuracy:1.0000.:  60%|█████▉    | 119/200 [01:38<01:01,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0503, train accuracy:1.0000.:  60%|██████    | 120/200 [01:38<00:48,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0477, train accuracy:1.0000.:  60%|██████    | 120/200 [01:40<00:48,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0477, train accuracy:1.0000.:  60%|██████    | 121/200 [01:40<01:33,  1.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0439, train accuracy:1.0000.:  60%|██████    | 121/200 [01:40<01:33,  1.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0439, train accuracy:1.0000.:  61%|██████    | 122/200 [01:40<01:09,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0397, train accuracy:1.0000.:  61%|██████    | 122/200 [01:41<01:09,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0397, train accuracy:1.0000.:  62%|██████▏   | 123/200 [01:41<00:53,  1.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0457, train accuracy:1.0000.:  62%|██████▏   | 123/200 [01:43<00:53,  1.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0457, train accuracy:1.0000.:  62%|██████▏   | 124/200 [01:43<01:21,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0388, train accuracy:1.0000.:  62%|██████▏   | 124/200 [01:43<01:21,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0388, train accuracy:1.0000.:  62%|██████▎   | 125/200 [01:43<01:01,  1.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0407, train accuracy:1.0000.:  62%|██████▎   | 125/200 [01:43<01:01,  1.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0407, train accuracy:1.0000.:  63%|██████▎   | 126/200 [01:43<00:47,  1.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0411, train accuracy:1.0000.:  63%|██████▎   | 126/200 [01:45<00:47,  1.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0411, train accuracy:1.0000.:  64%|██████▎   | 127/200 [01:45<01:14,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0468, train accuracy:1.0000.:  64%|██████▎   | 127/200 [01:45<01:14,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0468, train accuracy:1.0000.:  64%|██████▍   | 128/200 [01:45<00:56,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0456, train accuracy:1.0000.:  64%|██████▍   | 128/200 [01:45<00:56,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0456, train accuracy:1.0000.:  64%|██████▍   | 129/200 [01:45<00:43,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0488, train accuracy:1.0000.:  64%|██████▍   | 129/200 [01:47<00:43,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0488, train accuracy:1.0000.:  65%|██████▌   | 130/200 [01:47<01:09,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0501, train accuracy:1.0000.:  65%|██████▌   | 130/200 [01:48<01:09,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0501, train accuracy:1.0000.:  66%|██████▌   | 131/200 [01:48<00:52,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0411, train accuracy:1.0000.:  66%|██████▌   | 131/200 [01:48<00:52,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0411, train accuracy:1.0000.:  66%|██████▌   | 132/200 [01:48<00:40,  1.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0460, train accuracy:1.0000.:  66%|██████▌   | 132/200 [01:50<00:40,  1.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0460, train accuracy:1.0000.:  66%|██████▋   | 133/200 [01:50<01:06,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0382, train accuracy:1.0000.:  66%|██████▋   | 133/200 [01:50<01:06,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0382, train accuracy:1.0000.:  67%|██████▋   | 134/200 [01:50<00:49,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0501, train accuracy:1.0000.:  67%|██████▋   | 134/200 [01:50<00:49,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0501, train accuracy:1.0000.:  68%|██████▊   | 135/200 [01:50<00:38,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0474, train accuracy:1.0000.:  68%|██████▊   | 135/200 [01:52<00:38,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0474, train accuracy:1.0000.:  68%|██████▊   | 136/200 [01:52<01:03,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0371, train accuracy:1.0000.:  68%|██████▊   | 136/200 [01:52<01:03,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0371, train accuracy:1.0000.:  68%|██████▊   | 137/200 [01:52<00:47,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0415, train accuracy:1.0000.:  68%|██████▊   | 137/200 [01:52<00:47,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0415, train accuracy:1.0000.:  69%|██████▉   | 138/200 [01:52<00:36,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0366, train accuracy:1.0000.:  69%|██████▉   | 138/200 [01:54<00:36,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0366, train accuracy:1.0000.:  70%|██████▉   | 139/200 [01:54<01:00,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0385, train accuracy:1.0000.:  70%|██████▉   | 139/200 [01:55<01:00,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0385, train accuracy:1.0000.:  70%|███████   | 140/200 [01:55<00:45,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0412, train accuracy:1.0000.:  70%|███████   | 140/200 [01:55<00:45,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0412, train accuracy:1.0000.:  70%|███████   | 141/200 [01:55<00:46,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0377, train accuracy:1.0000.:  70%|███████   | 141/200 [01:57<00:46,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0377, train accuracy:1.0000.:  71%|███████   | 142/200 [01:57<01:05,  1.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0451, train accuracy:1.0000.:  71%|███████   | 142/200 [01:58<01:05,  1.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0451, train accuracy:1.0000.:  72%|███████▏  | 143/200 [01:58<00:48,  1.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0392, train accuracy:1.0000.:  72%|███████▏  | 143/200 [01:58<00:48,  1.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0392, train accuracy:1.0000.:  72%|███████▏  | 144/200 [01:58<00:37,  1.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0363, train accuracy:1.0000.:  72%|███████▏  | 144/200 [02:00<00:37,  1.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0363, train accuracy:1.0000.:  72%|███████▎  | 145/200 [02:00<00:57,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0431, train accuracy:1.0000.:  72%|███████▎  | 145/200 [02:00<00:57,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0431, train accuracy:1.0000.:  73%|███████▎  | 146/200 [02:00<00:43,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0374, train accuracy:1.0000.:  73%|███████▎  | 146/200 [02:00<00:43,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0374, train accuracy:1.0000.:  74%|███████▎  | 147/200 [02:00<00:33,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0393, train accuracy:1.0000.:  74%|███████▎  | 147/200 [02:02<00:33,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0393, train accuracy:1.0000.:  74%|███████▍  | 148/200 [02:02<00:52,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0322, train accuracy:1.0000.:  74%|███████▍  | 148/200 [02:02<00:52,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0322, train accuracy:1.0000.:  74%|███████▍  | 149/200 [02:02<00:39,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0305, train accuracy:1.0000.:  74%|███████▍  | 149/200 [02:03<00:39,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0305, train accuracy:1.0000.:  75%|███████▌  | 150/200 [02:03<00:30,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0349, train accuracy:1.0000.:  75%|███████▌  | 150/200 [02:05<00:30,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0349, train accuracy:1.0000.:  76%|███████▌  | 151/200 [02:05<00:50,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0310, train accuracy:1.0000.:  76%|███████▌  | 151/200 [02:05<00:50,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0310, train accuracy:1.0000.:  76%|███████▌  | 152/200 [02:05<00:37,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0337, train accuracy:1.0000.:  76%|███████▌  | 152/200 [02:05<00:37,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0337, train accuracy:1.0000.:  76%|███████▋  | 153/200 [02:05<00:28,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0327, train accuracy:1.0000.:  76%|███████▋  | 153/200 [02:07<00:28,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0327, train accuracy:1.0000.:  77%|███████▋  | 154/200 [02:07<00:46,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0289, train accuracy:1.0000.:  77%|███████▋  | 154/200 [02:07<00:46,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0289, train accuracy:1.0000.:  78%|███████▊  | 155/200 [02:07<00:34,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0449, train accuracy:1.0000.:  78%|███████▊  | 155/200 [02:07<00:34,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0449, train accuracy:1.0000.:  78%|███████▊  | 156/200 [02:07<00:26,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0373, train accuracy:1.0000.:  78%|███████▊  | 156/200 [02:09<00:26,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0373, train accuracy:1.0000.:  78%|███████▊  | 157/200 [02:09<00:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0321, train accuracy:1.0000.:  78%|███████▊  | 157/200 [02:09<00:42,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0321, train accuracy:1.0000.:  79%|███████▉  | 158/200 [02:09<00:32,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0336, train accuracy:1.0000.:  79%|███████▉  | 158/200 [02:10<00:32,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0336, train accuracy:1.0000.:  80%|███████▉  | 159/200 [02:10<00:24,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0336, train accuracy:1.0000.:  80%|███████▉  | 159/200 [02:12<00:24,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0336, train accuracy:1.0000.:  80%|████████  | 160/200 [02:12<00:40,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0299, train accuracy:1.0000.:  80%|████████  | 160/200 [02:13<00:40,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0299, train accuracy:1.0000.:  80%|████████  | 161/200 [02:13<00:37,  1.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0350, train accuracy:1.0000.:  80%|████████  | 161/200 [02:13<00:37,  1.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0350, train accuracy:1.0000.:  81%|████████  | 162/200 [02:13<00:28,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0337, train accuracy:1.0000.:  81%|████████  | 162/200 [02:15<00:28,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0337, train accuracy:1.0000.:  82%|████████▏ | 163/200 [02:15<00:40,  1.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0301, train accuracy:1.0000.:  82%|████████▏ | 163/200 [02:15<00:40,  1.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0301, train accuracy:1.0000.:  82%|████████▏ | 164/200 [02:15<00:30,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0306, train accuracy:1.0000.:  82%|████████▏ | 164/200 [02:15<00:30,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0306, train accuracy:1.0000.:  82%|████████▎ | 165/200 [02:15<00:22,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0354, train accuracy:1.0000.:  82%|████████▎ | 165/200 [02:17<00:22,  1.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0354, train accuracy:1.0000.:  83%|████████▎ | 166/200 [02:17<00:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0298, train accuracy:1.0000.:  83%|████████▎ | 166/200 [02:17<00:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0298, train accuracy:1.0000.:  84%|████████▎ | 167/200 [02:17<00:26,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0389, train accuracy:1.0000.:  84%|████████▎ | 167/200 [02:18<00:26,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0389, train accuracy:1.0000.:  84%|████████▍ | 168/200 [02:18<00:20,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0326, train accuracy:1.0000.:  84%|████████▍ | 168/200 [02:19<00:20,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0326, train accuracy:1.0000.:  84%|████████▍ | 169/200 [02:19<00:31,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0331, train accuracy:1.0000.:  84%|████████▍ | 169/200 [02:20<00:31,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0331, train accuracy:1.0000.:  85%|████████▌ | 170/200 [02:20<00:23,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0301, train accuracy:1.0000.:  85%|████████▌ | 170/200 [02:20<00:23,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0301, train accuracy:1.0000.:  86%|████████▌ | 171/200 [02:20<00:17,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0335, train accuracy:1.0000.:  86%|████████▌ | 171/200 [02:22<00:17,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0335, train accuracy:1.0000.:  86%|████████▌ | 172/200 [02:22<00:27,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0293, train accuracy:1.0000.:  86%|████████▌ | 172/200 [02:22<00:27,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0293, train accuracy:1.0000.:  86%|████████▋ | 173/200 [02:22<00:20,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0275, train accuracy:1.0000.:  86%|████████▋ | 173/200 [02:22<00:20,  1.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0275, train accuracy:1.0000.:  87%|████████▋ | 174/200 [02:22<00:15,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0250, train accuracy:1.0000.:  87%|████████▋ | 174/200 [02:24<00:15,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0250, train accuracy:1.0000.:  88%|████████▊ | 175/200 [02:24<00:24,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0264, train accuracy:1.0000.:  88%|████████▊ | 175/200 [02:24<00:24,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0264, train accuracy:1.0000.:  88%|████████▊ | 176/200 [02:24<00:18,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0240, train accuracy:1.0000.:  88%|████████▊ | 176/200 [02:25<00:18,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0240, train accuracy:1.0000.:  88%|████████▊ | 177/200 [02:25<00:13,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0260, train accuracy:1.0000.:  88%|████████▊ | 177/200 [02:27<00:13,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0260, train accuracy:1.0000.:  89%|████████▉ | 178/200 [02:27<00:22,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0290, train accuracy:1.0000.:  89%|████████▉ | 178/200 [02:27<00:22,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0290, train accuracy:1.0000.:  90%|████████▉ | 179/200 [02:27<00:16,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0322, train accuracy:1.0000.:  90%|████████▉ | 179/200 [02:27<00:16,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0322, train accuracy:1.0000.:  90%|█████████ | 180/200 [02:27<00:12,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0235, train accuracy:1.0000.:  90%|█████████ | 180/200 [02:30<00:12,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0235, train accuracy:1.0000.:  90%|█████████ | 181/200 [02:30<00:22,  1.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0229, train accuracy:1.0000.:  90%|█████████ | 181/200 [02:30<00:22,  1.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0229, train accuracy:1.0000.:  91%|█████████ | 182/200 [02:30<00:16,  1.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0254, train accuracy:1.0000.:  91%|█████████ | 182/200 [02:30<00:16,  1.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0254, train accuracy:1.0000.:  92%|█████████▏| 183/200 [02:30<00:11,  1.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0267, train accuracy:1.0000.:  92%|█████████▏| 183/200 [02:32<00:11,  1.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0267, train accuracy:1.0000.:  92%|█████████▏| 184/200 [02:32<00:17,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0224, train accuracy:1.0000.:  92%|█████████▏| 184/200 [02:32<00:17,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0224, train accuracy:1.0000.:  92%|█████████▎| 185/200 [02:32<00:12,  1.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0334, train accuracy:1.0000.:  92%|█████████▎| 185/200 [02:32<00:12,  1.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0334, train accuracy:1.0000.:  93%|█████████▎| 186/200 [02:32<00:08,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0225, train accuracy:1.0000.:  93%|█████████▎| 186/200 [02:34<00:08,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0225, train accuracy:1.0000.:  94%|█████████▎| 187/200 [02:34<00:13,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0212, train accuracy:1.0000.:  94%|█████████▎| 187/200 [02:34<00:13,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0212, train accuracy:1.0000.:  94%|█████████▍| 188/200 [02:34<00:09,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0289, train accuracy:1.0000.:  94%|█████████▍| 188/200 [02:35<00:09,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0289, train accuracy:1.0000.:  94%|█████████▍| 189/200 [02:35<00:06,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0250, train accuracy:1.0000.:  94%|█████████▍| 189/200 [02:37<00:06,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0250, train accuracy:1.0000.:  95%|█████████▌| 190/200 [02:37<00:10,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0249, train accuracy:1.0000.:  95%|█████████▌| 190/200 [02:37<00:10,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0249, train accuracy:1.0000.:  96%|█████████▌| 191/200 [02:37<00:06,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0291, train accuracy:1.0000.:  96%|█████████▌| 191/200 [02:37<00:06,  1.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0291, train accuracy:1.0000.:  96%|█████████▌| 192/200 [02:37<00:04,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0284, train accuracy:1.0000.:  96%|█████████▌| 192/200 [02:39<00:04,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0284, train accuracy:1.0000.:  96%|█████████▋| 193/200 [02:39<00:06,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0293, train accuracy:1.0000.:  96%|█████████▋| 193/200 [02:39<00:06,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0293, train accuracy:1.0000.:  97%|█████████▋| 194/200 [02:39<00:04,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0287, train accuracy:1.0000.:  97%|█████████▋| 194/200 [02:39<00:04,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0287, train accuracy:1.0000.:  98%|█████████▊| 195/200 [02:39<00:03,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0232, train accuracy:1.0000.:  98%|█████████▊| 195/200 [02:41<00:03,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0232, train accuracy:1.0000.:  98%|█████████▊| 196/200 [02:41<00:03,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0305, train accuracy:1.0000.:  98%|█████████▊| 196/200 [02:42<00:03,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0305, train accuracy:1.0000.:  98%|█████████▊| 197/200 [02:42<00:02,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0251, train accuracy:1.0000.:  98%|█████████▊| 197/200 [02:42<00:02,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0251, train accuracy:1.0000.:  99%|█████████▉| 198/200 [02:42<00:01,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0230, train accuracy:1.0000.:  99%|█████████▉| 198/200 [02:44<00:01,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0230, train accuracy:1.0000.: 100%|█████████▉| 199/200 [02:44<00:01,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0243, train accuracy:1.0000.: 100%|█████████▉| 199/200 [02:44<00:01,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train loss:0.0243, train accuracy:1.0000.: 100%|██████████| 200/200 [02:44<00:00,  1.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZItH2lX7k4Yt"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XAXHAUf3EEiE"
      },
      "source": [
        "##2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yu9usOxtjFHL"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_K7F19SPQo6U"
      },
      "source": [
        "### 2.1 Generate text from an the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9YLXvK51RnuL"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WDNOb_H5IRvH",
        "outputId": "566542c0-4077-49c3-c7aa-b8d71e86c065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=6acd0e3b220690805e75f6a1fe9ad74ee9f766fc926a2fbcbda8d4d32d3be873\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6aRJ-c9uRMOa",
        "outputId": "5df9a1ab-7714-40d5-9790-b156aae8df6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching checkpoint: 1.05Mit [00:00, 302Mit/s]                                                      \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 124M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching encoder.json: 1.05Mit [00:00, 98.6Mit/s]                                                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching hparams.json:   0%|                                            | 0.00/90.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching hparams.json: 1.05Mit [00:00, 429Mit/s]                                                    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:   1%|▏                | 5.24M/498M [00:00<00:22, 22.3Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:   4%|▌                | 17.8M/498M [00:00<00:20, 23.4Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:   5%|▉                | 26.2M/498M [00:00<00:16, 29.0Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  10%|█▊               | 51.4M/498M [00:01<00:11, 38.0Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  14%|██▎              | 68.2M/498M [00:01<00:09, 46.7Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  19%|███▏             | 93.3M/498M [00:01<00:06, 59.5Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  21%|███▊              | 104M/498M [00:01<00:06, 61.5Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  24%|████▎             | 121M/498M [00:01<00:04, 75.8Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  27%|████▉             | 135M/498M [00:01<00:04, 78.4Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  29%|█████▎            | 146M/498M [00:02<00:07, 45.2Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  36%|██████▍           | 177M/498M [00:02<00:05, 53.7Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  42%|███████▌          | 211M/498M [00:02<00:04, 65.0Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  48%|████████▌         | 237M/498M [00:02<00:03, 83.8Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  51%|█████████         | 252M/498M [00:03<00:03, 79.7Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  54%|█████████▋        | 269M/498M [00:03<00:02, 87.3Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  57%|██████████▏       | 282M/498M [00:03<00:02, 94.7Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  61%|███████████▌       | 303M/498M [00:03<00:01, 106Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  64%|████████████▏      | 320M/498M [00:03<00:01, 113Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  68%|████████████▊      | 337M/498M [00:03<00:01, 108Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  73%|█████████████▊     | 362M/498M [00:03<00:01, 120Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  75%|██████████████▎    | 375M/498M [00:04<00:01, 118Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  79%|██████████████▉    | 392M/498M [00:04<00:00, 129Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  82%|███████████████▌   | 407M/498M [00:04<00:00, 128Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  84%|████████████████   | 420M/498M [00:04<00:00, 122Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  88%|████████████████▋  | 437M/498M [00:04<00:00, 130Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  91%|█████████████████▎ | 454M/498M [00:04<00:00, 127Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  96%|██████████████████▏| 476M/498M [00:04<00:00, 142Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:  99%|██████████████████▊| 492M/498M [00:04<00:00, 124Mit/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 99.5Mit/s]                                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.index:   0%|                                       | 0.00/5.21k [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 504Mit/s]                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.meta:   0%|                                         | 0.00/471k [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 159Mit/s]                                                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching vocab.bpe:   0%|                                               | 0.00/456k [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 173Mit/s]                                                       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "U.S. ambassador to the United Nations, Nikki Haley, said Wednesday she expects the United States to play a role and \"suggest that this is not a question of US involvement.\"\n",
            "\n",
            "\"We do need to be viewed as a professional and diplomatic player in the world,\" Haley said in a statement. \"We also have to ensure that the United States has a role in helping to ensure that we're not in a position to do anything that's part of this process.\"\n",
            "\n",
            "By the time she leaves office in December, Trump will be facing an uncertain future after the UN Security Council unanimously passed a resolution supporting the United States to withdraw from the group.\n",
            "\n",
            "The resolution could do just that, according to Haley, who is expected to have a meeting with Trump at the White House this week.\n",
            "\n",
            "Related: Trump says he will not \"go through\" the UN vote\n",
            "\n",
            "The US has repeatedly said it would not use force to stop the Islamic State group from advancing into Iraq and Syria, claiming it has the ability to build a military presence there.\n",
            "\n",
            "The U.S. has already stepped up its military presence in Iraq and Syria in recent months, and has deployed a drone to support Iraqi forces battling Islamic State militants.\n",
            "\n",
            "On Wednesday, the United Nations Security Council passed a resolution supporting the United States to stay within the UN Security Council and force the government of Iraq to hand over power to the country's new Iraqi government.\n",
            "\n",
            "Following the vote, Haley said Washington would continue to support its troops and help the Iraqi government restore its control over its own government.\n",
            "\n",
            "\"I believe we should go through this process,\" she said. \"And we will, but the information I have from this vote is not going to be that way. I think we need to be very clear and clear as to what we're going to do next.\"\n",
            "\n",
            "She said the U.S. continued to provide air and ground support and a \"number of aircraft\" to Iraq, \"but we are not going to give up any power or any military base.\"\n",
            "\n",
            "In the meantime, Haley said she hoped to focus on \"training the Iraqis to be better leaders.\"\n",
            "\n",
            "\"I hope to see them become better leaders,\" she said. \"They are going to learn how to be better leaders in a way that they haven't been taught in the United States.\"<|endoftext|>\"I'm the one who sells your stuff. You're the one who sells sex.\"\n",
            "\n",
            "Motown is a sort of apocalypse. It takes place in a little town left devastated by the Great Depression, and the town is a veritable wasteland. The people look to the sky and the roads, and the people are so desperate for food, they don't even make it to the end of it. It is the kind of place where half the people are dead, and the other half are alive.\n",
            "\n",
            "\"You see what I mean?\"\n",
            "\n",
            "\"Yes.\"\n",
            "\n",
            "Pippin's house is an English-language bar and tavern, and Pippin is a busy guy. He is the type who regularly visits people he doesn't know, and that's where he meets the lady he tries to marry. He is that type of man who spends most of his time in town, he's a business guy, and he constantly has to deal with her.\n",
            "\n",
            "\"So, it's my turn to meet you,\" he says. \"I'm still a little bit on edge. Can you help me?\"\n",
            "\n",
            "\"Of course I can.\"\n",
            "\n",
            "\"I know, I know. But I gotta tell you, that's not the answer.\"\n",
            "\n",
            "\"What? Not the answer? You mean, that's the answer? You're not the answer?\"\n",
            "\n",
            "\"No, that's not the answer. Not the answer. You're the one who sells sex to me, and I'm the one who sells sex to you.\"\n",
            "\n",
            "He says it as if he is in a dream and he is so happy that he will never get to meet this girl again. That's how he confronts her.\n",
            "\n",
            "\"Are you serious?\"\n",
            "\n",
            "\"No, I'm just going to tell you. I like to talk to people. I like to talk to people. I like to talk to people.\"\n",
            "\n",
            "\"What you're talking about, is your right to know. I don't care what you think. I care what you think about me. I don't care what you think about me. I'm not the one who sells sex to you. I'm not the one who talks about sex. I'm not the one who talks about sex. I'm the one who talks about sex. I do not care what you think about me. I do not care what you think about me. I'm not the one who talks about sex.\"\n",
            "\n",
            "\"I'm getting back to you,\" Pippin says. \"You're leaving me. You're leaving me. Why? Why do you want to see me? Why do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AHmjSVf_FNHv"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gPXJkNubFyY6"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KWkuRjbcFzwb"
      },
      "source": [
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iD45m3IwF9hh"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ESltl2QM5nxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "c784d2bb33804f6b8bcd7119d1c6bd65",
            "5005e7b651f74dba97fb8515066be2fc",
            "8153ec76ee7144c58f2f1a2dcc812a37",
            "9d53355c481a471a84ccaf898e6439d4",
            "8bf49ed3b99641468fc8468ee0e42e2e",
            "31b9479c252c4a3ba6bb044dc53d4b12",
            "cc9f548958d0435280fb77ab213c7359",
            "5cebe2bc16044c1a95de5c76bcac62f8"
          ]
        },
        "outputId": "a306376f-87fe-4990-b5b5-c5d21a5c08bf"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "url = 'https://www.gutenberg.org/files/30/30.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/30.txt /data/text/bible.txt\n",
        "!ls ../data/text"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.gutenberg.org/files/30/30.zip to ../data/text.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c784d2bb33804f6b8bcd7119d1c6bd65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/text.zip\n",
            "bible.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "usQE-rSPZq_X"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGQ6-Vpmz4V7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a73a0a3-b883-4ea6-e667-8a014e7742cf"
      },
      "source": [
        "url = 'https://www.gutenberg.org/files/98/98.zip'\n",
        "download_dataset(url)\n",
        "# !mv /data/text/30.txt /data/text/bible.txt\n",
        "# !ls ../data/text"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../data/text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odbyZNr-GwN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "504978de-9723-4d1f-c434-2c458ebb1c99"
      },
      "source": [
        "! unzip 98.zip"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  98.zip\n",
            "  inflating: 98.txt                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKqvO0yHPlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a881ea89-914e-469a-8ef6-8c59aa2b034c"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.txt\tintel-image-classification\tmodels\n",
            "98.zip\tintel-image-classification.zip\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IoA0tZZCa_1k"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OoU6ML1mbgjP"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8pa5vFJ5EUjv"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WuQ5snl4LuS0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2c02b96-1867-4466-ef9c-b9bebf7670f4"
      },
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='./98.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 201008 tokens\n",
            "Training...\n",
            "[1 | 8.29] loss=3.33 avg=3.33\n",
            "[2 | 10.55] loss=3.50 avg=3.42\n",
            "[3 | 12.80] loss=3.53 avg=3.46\n",
            "[4 | 15.05] loss=3.36 avg=3.43\n",
            "[5 | 17.32] loss=3.61 avg=3.47\n",
            "[6 | 19.59] loss=3.16 avg=3.42\n",
            "[7 | 21.88] loss=3.43 avg=3.42\n",
            "[8 | 24.17] loss=3.60 avg=3.44\n",
            "[9 | 26.48] loss=3.42 avg=3.44\n",
            "[10 | 28.80] loss=3.45 avg=3.44\n",
            "[11 | 31.14] loss=3.26 avg=3.42\n",
            "[12 | 33.48] loss=3.38 avg=3.42\n",
            "[13 | 35.83] loss=3.08 avg=3.39\n",
            "[14 | 38.21] loss=3.40 avg=3.39\n",
            "[15 | 40.59] loss=3.01 avg=3.36\n",
            "[16 | 42.98] loss=3.27 avg=3.36\n",
            "[17 | 45.39] loss=3.36 avg=3.36\n",
            "[18 | 47.81] loss=3.27 avg=3.35\n",
            "[19 | 50.24] loss=3.45 avg=3.36\n",
            "[20 | 52.66] loss=3.33 avg=3.36\n",
            "[21 | 55.10] loss=3.29 avg=3.35\n",
            "[22 | 57.51] loss=3.44 avg=3.36\n",
            "[23 | 59.89] loss=3.05 avg=3.34\n",
            "[24 | 62.28] loss=3.52 avg=3.35\n",
            "[25 | 64.65] loss=3.31 avg=3.35\n",
            "[26 | 67.01] loss=3.08 avg=3.34\n",
            "[27 | 69.36] loss=3.19 avg=3.33\n",
            "[28 | 71.71] loss=2.69 avg=3.31\n",
            "[29 | 74.05] loss=3.29 avg=3.30\n",
            "[30 | 76.39] loss=3.20 avg=3.30\n",
            "[31 | 78.72] loss=3.18 avg=3.30\n",
            "[32 | 81.04] loss=3.18 avg=3.29\n",
            "[33 | 83.36] loss=3.19 avg=3.29\n",
            "[34 | 85.68] loss=3.13 avg=3.28\n",
            "[35 | 88.01] loss=3.32 avg=3.28\n",
            "[36 | 90.32] loss=3.19 avg=3.28\n",
            "[37 | 92.64] loss=3.07 avg=3.27\n",
            "[38 | 94.95] loss=2.72 avg=3.26\n",
            "[39 | 97.26] loss=3.05 avg=3.25\n",
            "[40 | 99.57] loss=3.26 avg=3.25\n",
            "[41 | 101.89] loss=3.16 avg=3.25\n",
            "[42 | 104.20] loss=3.22 avg=3.25\n",
            "[43 | 106.53] loss=3.12 avg=3.24\n",
            "[44 | 108.86] loss=3.09 avg=3.24\n",
            "[45 | 111.20] loss=3.10 avg=3.24\n",
            "[46 | 113.54] loss=2.60 avg=3.22\n",
            "[47 | 115.89] loss=3.14 avg=3.22\n",
            "[48 | 118.24] loss=3.01 avg=3.21\n",
            "[49 | 120.60] loss=3.16 avg=3.21\n",
            "[50 | 122.97] loss=3.21 avg=3.21\n",
            "[51 | 125.32] loss=2.97 avg=3.20\n",
            "[52 | 127.68] loss=3.00 avg=3.20\n",
            "[53 | 130.05] loss=3.02 avg=3.19\n",
            "[54 | 132.42] loss=3.09 avg=3.19\n",
            "[55 | 134.79] loss=3.16 avg=3.19\n",
            "[56 | 137.14] loss=2.96 avg=3.19\n",
            "[57 | 139.50] loss=3.02 avg=3.18\n",
            "[58 | 141.85] loss=3.12 avg=3.18\n",
            "[59 | 144.20] loss=3.11 avg=3.18\n",
            "[60 | 146.55] loss=3.22 avg=3.18\n",
            "[61 | 148.90] loss=3.07 avg=3.18\n",
            "[62 | 151.25] loss=3.11 avg=3.18\n",
            "[63 | 153.60] loss=2.90 avg=3.17\n",
            "[64 | 155.95] loss=2.94 avg=3.16\n",
            "[65 | 158.30] loss=2.28 avg=3.15\n",
            "[66 | 160.65] loss=3.19 avg=3.15\n",
            "[67 | 162.99] loss=3.00 avg=3.14\n",
            "[68 | 165.34] loss=2.83 avg=3.14\n",
            "[69 | 167.67] loss=2.71 avg=3.13\n",
            "[70 | 170.01] loss=2.82 avg=3.12\n",
            "[71 | 172.36] loss=2.95 avg=3.12\n",
            "[72 | 174.69] loss=3.08 avg=3.12\n",
            "[73 | 177.04] loss=3.19 avg=3.12\n",
            "[74 | 179.37] loss=3.18 avg=3.12\n",
            "[75 | 181.71] loss=3.00 avg=3.12\n",
            "[76 | 184.05] loss=2.96 avg=3.12\n",
            "[77 | 186.39] loss=3.09 avg=3.12\n",
            "[78 | 188.74] loss=2.81 avg=3.11\n",
            "[79 | 191.08] loss=2.99 avg=3.11\n",
            "[80 | 193.42] loss=2.41 avg=3.10\n",
            "[81 | 195.76] loss=2.89 avg=3.09\n",
            "[82 | 198.11] loss=2.57 avg=3.08\n",
            "[83 | 200.46] loss=2.80 avg=3.08\n",
            "[84 | 202.80] loss=2.96 avg=3.08\n",
            "[85 | 205.14] loss=3.03 avg=3.07\n",
            "[86 | 207.49] loss=2.27 avg=3.06\n",
            "[87 | 209.83] loss=2.97 avg=3.06\n",
            "[88 | 212.19] loss=3.03 avg=3.06\n",
            "[89 | 214.54] loss=2.96 avg=3.06\n",
            "[90 | 216.90] loss=3.14 avg=3.06\n",
            "[91 | 219.26] loss=2.93 avg=3.06\n",
            "[92 | 221.61] loss=3.06 avg=3.06\n",
            "[93 | 223.96] loss=2.76 avg=3.05\n",
            "[94 | 226.31] loss=2.86 avg=3.05\n",
            "[95 | 228.68] loss=2.98 avg=3.05\n",
            "[96 | 231.03] loss=2.49 avg=3.04\n",
            "[97 | 233.39] loss=2.88 avg=3.04\n",
            "[98 | 235.75] loss=2.81 avg=3.03\n",
            "[99 | 238.10] loss=2.90 avg=3.03\n",
            "[100 | 240.45] loss=2.62 avg=3.02\n",
            "======== SAMPLE 1 ========\n",
            " to the door of his own house. His sister then asked him, 'Do you know you have to go?' \"\n",
            "\n",
            "\"I do not know. I am sorry that I should tell you now, Mr. Lorry. What do you tell yourself? Have I told you? Are I not yet sure?\"\n",
            "\n",
            "\"I am so glad of it. It's wonderful to take so much time to think and to be done; to do it better, is to be so taken up with it as to be no more. But it is hard, and it is hard to be done, and it is hard and it is hard to see.\n",
            "\"I am sorry, dear Lorry, that you think, and have thought too much. I am sorry that my wife cannot think of\n",
            "the way out, and that I did not think of what I could do to save it. I am sorry and sorry that a young man so weak and so weak has been so difficult to save.\n",
            "\n",
            "\"I am sorry to you, and to all the women who have touched you, if anything will make it worse. I am sorry to them of all those\n",
            "women. I thank Heaven for them all. For them I say, for ever and ever, in my heart.\"\n",
            "\n",
            "That very same morning, Charles Darnay returned from Paris. His wife had not received her\n",
            "fellow-wife, however, but her husband had made arrangements for supper, and that he\n",
            "had left the house by the time it was evening.\n",
            "\n",
            "The Darnays had always been so kind to him, and for whom they had\n",
            "so little in common, and so much in common, that he was a man of many cares,\n",
            "and aspirations, and a man of short years, and of great ambition;\n",
            "and he was well known to many, even by their own ears, and was a\n",
            "gentleness and benevolence that brought him no satisfaction.\n",
            "\n",
            "On the first day of the month of November, on the\n",
            "night of his tenth birthday, he had received by letters the following letter\n",
            "from Mr. Lorry, answering a single letter, from Mr. Stryver,\n",
            "in which he expressed the wish to write to Mr. Carton, and to ask his\n",
            "assent and consent to their having it delivered. The day was early\n",
            "as they were riding out when they began the journey. The carriage in which they were\n",
            "walking had been delivered to Mr. Carton, and was awaiting it in an open\n",
            "door. There he remained in the carriage, with his father and mother and\n",
            "daughter, in company with him, until he sat down to supper and began. As his father and\n",
            "daughter were quite lost in wonder at his absence, no one could have\n",
            "seen him, and Mr. Lorry could not have known, that he had stood at Mr.\n",
            "Stryver's back, looking at him with a determined interest.\n",
            "\n",
            "\"Do you not understand,\" the father said, \"that my goodness, he must have\n",
            "been so intent upon seeing me?\"\n",
            "\n",
            "\"The truth is, I do not see him,\" said Mr. Lorry, \"unless it suits me better to\n",
            "say so. He had been so intent on seeing me, ever since I had\n",
            "come to him. I do not hear him calling my name, nor hear him\n",
            "knitting my name, until it suits me better to say what I am. He had\n",
            "been so intent on seeing me, ever since I had come to him; I see it. I am\n",
            "so glad that it is, and to think it will not be. I am so relieved\n",
            "that I do not wonder at it. I cannot do it, I miss it, miss it so much,\n",
            "no more, save that it will never be able to do it in my breast; in my thoughts,\n",
            "I miss it, miss it so much.\"\n",
            "\n",
            "\"I thank God,\" answered the father, \"that, being so well intent upon\n",
            "me, and feeling so happy that I should feel like a son to myself, I had never\n",
            "thought of doing it. But I miss it. My mind has been so much the same, the more\n",
            "so, that we have a strong connection; I have been so intent on it. I find it\n",
            "unexpected as a child; yet, too, I have been so intent upon it. For every thing that I know,\n",
            "and believe to be true, that might or might not be true, I am so\n",
            "intrusted with the details. I have had but little of my work, and my hands have\n",
            "been stretched out, and I have been so long with my hands but that I may have\n",
            "nothing in them, that I might think of nothing. That I had not the thought to\n",
            "say so. I know that I have an inclination towards saying it; that is why I\n",
            "am so anxious that I should be able to make\n",
            "\n",
            "[101 | 256.21] loss=2.67 avg=3.02\n",
            "[102 | 258.56] loss=3.01 avg=3.02\n",
            "[103 | 260.92] loss=2.77 avg=3.01\n",
            "[104 | 263.27] loss=2.29 avg=3.00\n",
            "[105 | 265.62] loss=2.78 avg=3.00\n",
            "[106 | 267.99] loss=3.01 avg=3.00\n",
            "[107 | 270.36] loss=2.88 avg=3.00\n",
            "[108 | 272.73] loss=2.95 avg=3.00\n",
            "[109 | 275.09] loss=2.91 avg=3.00\n",
            "[110 | 277.45] loss=2.88 avg=2.99\n",
            "[111 | 279.81] loss=2.80 avg=2.99\n",
            "[112 | 282.17] loss=2.87 avg=2.99\n",
            "[113 | 284.55] loss=2.74 avg=2.99\n",
            "[114 | 286.90] loss=2.53 avg=2.98\n",
            "[115 | 289.27] loss=3.05 avg=2.98\n",
            "[116 | 291.64] loss=2.83 avg=2.98\n",
            "[117 | 293.99] loss=2.04 avg=2.96\n",
            "[118 | 296.35] loss=2.83 avg=2.96\n",
            "[119 | 298.71] loss=2.77 avg=2.96\n",
            "[120 | 301.07] loss=2.73 avg=2.96\n",
            "[121 | 303.44] loss=2.70 avg=2.95\n",
            "[122 | 305.80] loss=2.68 avg=2.95\n",
            "[123 | 308.16] loss=2.15 avg=2.94\n",
            "[124 | 310.51] loss=2.78 avg=2.94\n",
            "[125 | 312.87] loss=2.79 avg=2.93\n",
            "[126 | 315.23] loss=2.94 avg=2.93\n",
            "[127 | 317.58] loss=2.82 avg=2.93\n",
            "[128 | 319.93] loss=2.92 avg=2.93\n",
            "[129 | 322.29] loss=2.63 avg=2.93\n",
            "[130 | 324.63] loss=2.76 avg=2.93\n",
            "[131 | 326.99] loss=2.78 avg=2.92\n",
            "[132 | 329.35] loss=2.77 avg=2.92\n",
            "[133 | 331.70] loss=2.90 avg=2.92\n",
            "[134 | 334.04] loss=2.90 avg=2.92\n",
            "[135 | 336.40] loss=2.48 avg=2.91\n",
            "[136 | 338.76] loss=2.50 avg=2.91\n",
            "[137 | 341.12] loss=2.77 avg=2.91\n",
            "[138 | 343.47] loss=2.53 avg=2.90\n",
            "[139 | 345.83] loss=2.90 avg=2.90\n",
            "[140 | 348.18] loss=2.81 avg=2.90\n",
            "[141 | 350.53] loss=2.80 avg=2.90\n",
            "[142 | 352.88] loss=2.44 avg=2.89\n",
            "[143 | 355.23] loss=2.59 avg=2.89\n",
            "[144 | 357.58] loss=2.41 avg=2.88\n",
            "[145 | 359.93] loss=2.81 avg=2.88\n",
            "[146 | 362.30] loss=2.54 avg=2.88\n",
            "[147 | 364.65] loss=2.67 avg=2.88\n",
            "[148 | 367.00] loss=2.45 avg=2.87\n",
            "[149 | 369.35] loss=1.48 avg=2.85\n",
            "[150 | 371.71] loss=2.85 avg=2.85\n",
            "[151 | 374.06] loss=2.10 avg=2.84\n",
            "[152 | 376.40] loss=2.50 avg=2.84\n",
            "[153 | 378.76] loss=2.53 avg=2.83\n",
            "[154 | 381.11] loss=2.50 avg=2.83\n",
            "[155 | 383.47] loss=2.44 avg=2.82\n",
            "[156 | 385.81] loss=2.75 avg=2.82\n",
            "[157 | 388.17] loss=1.73 avg=2.81\n",
            "[158 | 390.51] loss=2.66 avg=2.81\n",
            "[159 | 392.86] loss=2.56 avg=2.80\n",
            "[160 | 395.21] loss=2.54 avg=2.80\n",
            "[161 | 397.56] loss=2.52 avg=2.80\n",
            "[162 | 399.92] loss=2.92 avg=2.80\n",
            "[163 | 402.27] loss=2.58 avg=2.80\n",
            "[164 | 404.63] loss=2.45 avg=2.79\n",
            "[165 | 406.98] loss=2.54 avg=2.79\n",
            "[166 | 409.35] loss=2.61 avg=2.79\n",
            "[167 | 411.70] loss=2.28 avg=2.78\n",
            "[168 | 414.05] loss=2.73 avg=2.78\n",
            "[169 | 416.40] loss=2.60 avg=2.78\n",
            "[170 | 418.76] loss=2.39 avg=2.77\n",
            "[171 | 421.12] loss=2.40 avg=2.77\n",
            "[172 | 423.47] loss=2.38 avg=2.76\n",
            "[173 | 425.81] loss=2.18 avg=2.76\n",
            "[174 | 428.17] loss=2.52 avg=2.75\n",
            "[175 | 430.52] loss=2.07 avg=2.75\n",
            "[176 | 432.88] loss=2.55 avg=2.74\n",
            "[177 | 435.24] loss=2.62 avg=2.74\n",
            "[178 | 437.59] loss=2.11 avg=2.73\n",
            "[179 | 439.94] loss=2.42 avg=2.73\n",
            "[180 | 442.30] loss=2.30 avg=2.73\n",
            "[181 | 444.65] loss=2.49 avg=2.72\n",
            "[182 | 447.01] loss=2.50 avg=2.72\n",
            "[183 | 449.38] loss=2.42 avg=2.72\n",
            "[184 | 451.72] loss=2.31 avg=2.71\n",
            "[185 | 454.08] loss=2.52 avg=2.71\n",
            "[186 | 456.43] loss=2.16 avg=2.70\n",
            "[187 | 458.78] loss=2.61 avg=2.70\n",
            "[188 | 461.13] loss=1.91 avg=2.69\n",
            "[189 | 463.48] loss=2.44 avg=2.69\n",
            "[190 | 465.84] loss=2.30 avg=2.68\n",
            "[191 | 468.19] loss=1.94 avg=2.68\n",
            "[192 | 470.54] loss=2.59 avg=2.68\n",
            "[193 | 472.90] loss=2.47 avg=2.67\n",
            "[194 | 475.25] loss=2.19 avg=2.67\n",
            "[195 | 477.60] loss=2.50 avg=2.67\n",
            "[196 | 479.97] loss=2.29 avg=2.66\n",
            "[197 | 482.33] loss=2.11 avg=2.65\n",
            "[198 | 484.69] loss=2.32 avg=2.65\n",
            "[199 | 487.04] loss=2.31 avg=2.65\n",
            "[200 | 489.39] loss=2.42 avg=2.64\n",
            "======== SAMPLE 1 ========\n",
            " comes from.\n",
            "\"They called the coach to-night, as if there were one there who\n",
            "was not dead.\" That is why the coach was put in service.\n",
            "\n",
            "\"It is a misspent servant. I know of no such one. He is\n",
            "born in Paris, in England, and dies in France. He was a son of a\n",
            "poor boy, who worked for Mr. Jarvis Lorry, then a\n",
            "gentleman there, and still living. I speak English well at this\n",
            "time, and believe that he has the capacity and will to bring it to the light of\n",
            "all trades--as it is in the English tongue.) He was, at that time\n",
            "known as John the Confessor, and willed his child out of it.\n",
            "That servant's name was Charles Darnay.\"\n",
            "\n",
            "\"And what do they say? Tell me the truth!\" said Carton; \"if you are\n",
            "informed--if you follow the chain--how many years do they say he\n",
            "never died?\"\n",
            "\n",
            "\"Two hundred and ten, at least.\"\n",
            "\n",
            "\"Thirty-two years, and he was so far buried?\"\n",
            "\n",
            "\"Thirty years, and he is buried at St. Pancras's. I say, that's\n",
            "a long time.\"\n",
            "\n",
            "\"But there is no burying him?\"\n",
            "\n",
            "\"That's a mistake made. Twenty-two years, and the child of\n",
            "Mr. Lorry, and she is his wife, and they are buried together.\"\n",
            "\"Now, I have no doubt of their being ever buried together,\" said\n",
            "Carton. \"It must be a mistake, at least.\"\n",
            "\n",
            "\"If it was a mistake, it must be repeated,\" said Mr. Lorry. \"I want no\n",
            "information.\"\n",
            "\n",
            "\"But there is no burying him, then?\"\n",
            "\n",
            "\"No, no. What is the matter?\"\n",
            "\n",
            "\"It cannot be known, at this hour of the day, how much the\n",
            "child, now grown to eight, has grown to. I will give you now, as\n",
            "a matter of business, the year he was born.\"\n",
            "\n",
            "\"The year is twenty-two years.\"\n",
            "\n",
            "\"I believe, Mr. Monseigneur. But you have the information which you were\n",
            "exploring.\"\n",
            "\n",
            "\"I have the information, Mr. Carton.\"\n",
            "\n",
            "\"You have the information, Mr. Carton?\n",
            "I have buried him.\"\n",
            "\n",
            "\"I believe so, and with all due respect to that servant's name,\n",
            "I have buried his child too.\"\n",
            "\n",
            "\"And you have buried her child too?\"\n",
            "\n",
            "\"I believe this, and all those years.\"\n",
            "\n",
            "\"Now, I ask you to look here, and examine this thing. Is this\n",
            "a mistake, or not a mistake?\"\n",
            "\n",
            "\"It depends upon what question you ask.\"\n",
            "\n",
            "\"If you ask it, I believe you would be very hard pressed to find any\n",
            "answer to it. But if you ask it--\"\n",
            "\n",
            "\"I will leave you to it for the time being. I have got the object of\n",
            "hanging up the door-post. You ask my question, and I have taken the outer\n",
            "glass. The man lies on his back; I have put my hand upon the wound in the\n",
            "body, and taken the rest of the loose and loose again.\"\n",
            "\n",
            "Carton, now with the folded paper on his other hand, as if to pry open\n",
            "his throat, and say, in a tone of soft confidence:\n",
            "\n",
            "\"How is this done?\"\n",
            "\n",
            "\"By drawing a line in the sand. It is done. And the wound is gone. That\n",
            "sir, it is the wound of a miserable parent, who had to be turned\n",
            "into a useful and useful slave by the time he was old enough to\n",
            "have his own. So far, we have always been guided by a fear that\n",
            "the wound would be made by some other parent, and would then have to\n",
            "pass through the earth as an obligation. Then I put the hammer to his head,\n",
            "and made an arrangement for him to draw on that instrument, which had never\n",
            "been done before, from the floor; and he drew. Now, I have buried\n",
            "his child too, that I have tied a rope as close to his neck as\n",
            "I can. I have held him down, and he has dug himself down, and dug\n",
            "out. You ask what is done?\"\n",
            "\n",
            "\"What I say now. I have made him dig out. I have brought the man\n",
            "down to a trench at the lower gate of the House on the way to-day. The\n",
            "interior gate has been opened, and we are now at the Bank.\"\n",
            "\n",
            "\"Tell me the secret,\" said Carton, shaking his head; \"how can anything really\n",
            "consequently be revealed to the public? But it cannot be! Look\n",
            "at that\n",
            "\n",
            "[201 | 504.33] loss=2.53 avg=2.64\n",
            "[202 | 506.67] loss=2.32 avg=2.64\n",
            "[203 | 509.02] loss=2.19 avg=2.63\n",
            "[204 | 511.37] loss=2.22 avg=2.63\n",
            "[205 | 513.72] loss=2.28 avg=2.63\n",
            "[206 | 516.07] loss=1.90 avg=2.62\n",
            "[207 | 518.42] loss=2.43 avg=2.61\n",
            "[208 | 520.78] loss=2.29 avg=2.61\n",
            "[209 | 523.13] loss=2.37 avg=2.61\n",
            "[210 | 525.48] loss=2.32 avg=2.61\n",
            "[211 | 527.83] loss=1.69 avg=2.59\n",
            "[212 | 530.20] loss=2.21 avg=2.59\n",
            "[213 | 532.55] loss=2.55 avg=2.59\n",
            "[214 | 534.93] loss=1.59 avg=2.58\n",
            "[215 | 537.29] loss=2.29 avg=2.58\n",
            "[216 | 539.66] loss=2.13 avg=2.57\n",
            "[217 | 542.02] loss=2.25 avg=2.57\n",
            "[218 | 544.40] loss=1.79 avg=2.56\n",
            "[219 | 546.77] loss=2.31 avg=2.56\n",
            "[220 | 549.14] loss=2.27 avg=2.55\n",
            "[221 | 551.51] loss=2.29 avg=2.55\n",
            "[222 | 553.87] loss=2.10 avg=2.54\n",
            "[223 | 556.25] loss=2.17 avg=2.54\n",
            "[224 | 558.61] loss=2.20 avg=2.54\n",
            "[225 | 560.97] loss=2.09 avg=2.53\n",
            "[226 | 563.33] loss=2.00 avg=2.53\n",
            "[227 | 565.67] loss=2.20 avg=2.52\n",
            "[228 | 568.03] loss=1.99 avg=2.52\n",
            "[229 | 570.39] loss=2.10 avg=2.51\n",
            "[230 | 572.75] loss=1.99 avg=2.51\n",
            "[231 | 575.09] loss=1.91 avg=2.50\n",
            "[232 | 577.44] loss=2.49 avg=2.50\n",
            "[233 | 579.79] loss=1.87 avg=2.49\n",
            "[234 | 582.14] loss=2.19 avg=2.49\n",
            "[235 | 584.49] loss=2.29 avg=2.49\n",
            "[236 | 586.84] loss=1.91 avg=2.48\n",
            "[237 | 589.19] loss=2.50 avg=2.48\n",
            "[238 | 591.55] loss=1.74 avg=2.47\n",
            "[239 | 593.91] loss=2.34 avg=2.47\n",
            "[240 | 596.27] loss=1.95 avg=2.46\n",
            "[241 | 598.62] loss=2.01 avg=2.46\n",
            "[242 | 600.97] loss=1.62 avg=2.45\n",
            "[243 | 603.32] loss=2.45 avg=2.45\n",
            "[244 | 605.68] loss=1.77 avg=2.44\n",
            "[245 | 608.03] loss=1.73 avg=2.44\n",
            "[246 | 610.38] loss=2.20 avg=2.43\n",
            "[247 | 612.73] loss=1.94 avg=2.43\n",
            "[248 | 615.08] loss=1.79 avg=2.42\n",
            "[249 | 617.44] loss=2.36 avg=2.42\n",
            "[250 | 619.78] loss=1.93 avg=2.41\n",
            "[251 | 622.14] loss=2.02 avg=2.41\n",
            "[252 | 624.48] loss=1.80 avg=2.40\n",
            "[253 | 626.84] loss=1.22 avg=2.39\n",
            "[254 | 629.18] loss=1.75 avg=2.38\n",
            "[255 | 631.54] loss=2.18 avg=2.38\n",
            "[256 | 633.88] loss=1.78 avg=2.37\n",
            "[257 | 636.23] loss=2.06 avg=2.37\n",
            "[258 | 638.58] loss=2.17 avg=2.37\n",
            "[259 | 640.93] loss=1.76 avg=2.36\n",
            "[260 | 643.28] loss=1.76 avg=2.36\n",
            "[261 | 645.63] loss=1.96 avg=2.35\n",
            "[262 | 647.98] loss=2.05 avg=2.35\n",
            "[263 | 650.34] loss=1.89 avg=2.34\n",
            "[264 | 652.70] loss=1.57 avg=2.34\n",
            "[265 | 655.05] loss=1.57 avg=2.33\n",
            "[266 | 657.41] loss=1.89 avg=2.32\n",
            "[267 | 659.76] loss=2.08 avg=2.32\n",
            "[268 | 662.11] loss=1.60 avg=2.31\n",
            "[269 | 664.46] loss=1.87 avg=2.31\n",
            "[270 | 666.82] loss=1.55 avg=2.30\n",
            "[271 | 669.16] loss=1.77 avg=2.29\n",
            "[272 | 671.52] loss=1.79 avg=2.29\n",
            "[273 | 673.86] loss=2.05 avg=2.29\n",
            "[274 | 676.22] loss=1.87 avg=2.28\n",
            "[275 | 678.57] loss=1.88 avg=2.28\n",
            "[276 | 680.94] loss=1.50 avg=2.27\n",
            "[277 | 683.30] loss=2.06 avg=2.27\n",
            "[278 | 685.65] loss=1.71 avg=2.26\n",
            "[279 | 688.00] loss=2.22 avg=2.26\n",
            "[280 | 690.35] loss=2.03 avg=2.26\n",
            "[281 | 692.69] loss=1.81 avg=2.25\n",
            "[282 | 695.05] loss=1.68 avg=2.25\n",
            "[283 | 697.40] loss=1.85 avg=2.24\n",
            "[284 | 699.77] loss=1.47 avg=2.23\n",
            "[285 | 702.13] loss=1.96 avg=2.23\n",
            "[286 | 704.47] loss=2.08 avg=2.23\n",
            "[287 | 706.82] loss=1.49 avg=2.22\n",
            "[288 | 709.17] loss=1.93 avg=2.22\n",
            "[289 | 711.52] loss=1.83 avg=2.21\n",
            "[290 | 713.89] loss=1.55 avg=2.21\n",
            "[291 | 716.26] loss=1.97 avg=2.21\n",
            "[292 | 718.61] loss=1.63 avg=2.20\n",
            "[293 | 720.96] loss=2.34 avg=2.20\n",
            "[294 | 723.31] loss=1.84 avg=2.20\n",
            "[295 | 725.68] loss=1.59 avg=2.19\n",
            "[296 | 728.05] loss=2.10 avg=2.19\n",
            "[297 | 730.40] loss=1.47 avg=2.18\n",
            "[298 | 732.76] loss=1.71 avg=2.18\n",
            "[299 | 735.12] loss=1.29 avg=2.17\n",
            "[300 | 737.48] loss=1.33 avg=2.16\n",
            "======== SAMPLE 1 ========\n",
            " Madame.\n",
            "\"Miss Stryver, my friend,\" said Mr. Stryver, smiling, \"it is a pity that you\n",
            "can't eat at Monsieur's side; you are very weak.\"\n",
            "\n",
            "\"Pooh!\" cried Miss Pross; \"_you_ very _that_. I shall have no hope of\n",
            "getting out of it.\"\n",
            "\n",
            "But at that moment he added, with a smile:\n",
            "\n",
            "\"You are welcome, dear Pross, but only as your\n",
            "own personal guard.\"\n",
            "\n",
            "\"There is a law that says that the head shall never be upon Monsieur\n",
            "Monsieur's left breast,\" said Miss Pross. \"You do not understand\n",
            "that?\"\n",
            "\n",
            "\"Monsieur,\" said Mr. Stryver, firmly and forcibly, \"I do not understand it.\n",
            "Take your seat at the window, and let me help Monsieur Mr.\n",
            "--Tell Mr. Stryver to seat himself.\"\n",
            "\n",
            "\"You have not been recognised by Miss Pross?\"\n",
            "\n",
            "\"Monsieur,\" said Miss Pross, turning towards the door, \"I have been.\"\n",
            "\n",
            "As the old lady put her arm on his, and looked upward in her\n",
            "attention to her, Miss Pross observed her turning away a moment in\n",
            "conversation, as she always has done.\n",
            "\n",
            "After a long silence, he answered:\n",
            "\n",
            "\"The Doctor is a little wounded.\"\n",
            "\n",
            "\"He might have been,\" said Miss Pross, with a radiant smile.\n",
            "\n",
            "\"Into the face or the message?\"\n",
            "\n",
            "\"Yes, sir.\"\n",
            "\n",
            "\"And the wound?\"\n",
            "\n",
            "\"There is a dreadful face there--a dreadful face, sir.\"\n",
            "\n",
            "\"And the message?\"\n",
            "\n",
            "\"Yes, sir.\"\n",
            "\n",
            "\"Monsieur, I am so ill as to have a dreadful face.\"\n",
            "\n",
            "\"Monsieur, I am so ill as _I_ have a dreadful face as a\n",
            "mere boy. Who, having seen the dreadful face before, should advise\n",
            "her to see it again? What danger can Monsieur the\n",
            "Farewell, my dear Miss Pross? Who is Monseigneur now?\"\n",
            "\n",
            "\"The speaker,\" said Miss Pross, recovering her former composure, \"Monsieur\n",
            "Evremonde.\"\n",
            "\n",
            "\"Monseigneur!\" said Miss Pross, \"he was not to be expected near\n",
            "the present moment. Now, may I ask the matter a little more propoundedly\n",
            "and argumentatively. Can Monsieur Mr. Stryver be trusted to the delicate\n",
            "business of rendering assistance to these miserable victims whom\n",
            "he has rendered service with most courage, when the best opportunity\n",
            "is--?\"\n",
            "\n",
            "\"Monsieur, I am sensible the former condition, and highly suspect the latter\n",
            "conditional,--he is at work to examine the remains of this\n",
            "barbarous body.\"\n",
            "\n",
            "\"Monsieur, I am sensible it may involve the life and limb of the\n",
            "man.\"\n",
            "\n",
            "\"--Monsieur, I am sensible is the case.--\"\n",
            "\n",
            "\"--Which, for the moment, I shall refer to as long as there\n",
            "is life. He is, therefore, asked to render his assistance in\n",
            "the examination of this dreary and impassable region.\"\n",
            "\n",
            "\"Monsieur, I am sensible the latter condition is the case. He is\n",
            "departed from the surrounding region by the side of the road,\n",
            "and needs the assistance of a short road, in which he has\n",
            "passaged a long time. In short, may Monsieur Mr. Stryver be allowed\n",
            "to remain?\"\n",
            "\n",
            "\"Monsieur, I am sensible the latter is the case.\"\n",
            "\n",
            "\"Monseigneur!\" said Miss Pross, \"I reiterate my confidence in your\n",
            "faithfulness to the Lord, and in the efficacy of his mercies. Indeed,\n",
            "I thank the Lord for this privilege. When will you be permitted to remain?\"\n",
            "\n",
            "\"Monseigneur?\"\n",
            "\n",
            "\"Monseigneur, now.\"\n",
            "\n",
            "\"And now!\" cried Miss Pross, shaking her head and\n",
            "tearfully shaking her hair.\n",
            "\n",
            "\"Miss Pross, that is a frightful thing. The Doctor has left\n",
            "him here.\"\n",
            "\n",
            "\"Monsieur, I am sensible it is a dreadful thing to return to him\n",
            "now, or even half a year from now.\"\n",
            "\n",
            "\"A rather dreadful thing,\" said Miss Pross, shaking her head. \"If it\n",
            "was not already so, it would be amiss for me to admit that he may\n",
            "have left her long ago. If it was not already so, it would be worse\n",
            "than committing suicide. But, my dear Miss, it is better for me to admit\n",
            "that--than not admit it\n",
            "\n",
            "[301 | 752.35] loss=1.46 avg=2.15\n",
            "[302 | 754.69] loss=1.52 avg=2.14\n",
            "[303 | 757.04] loss=1.69 avg=2.14\n",
            "[304 | 759.39] loss=1.97 avg=2.14\n",
            "[305 | 761.74] loss=1.81 avg=2.13\n",
            "[306 | 764.09] loss=1.51 avg=2.13\n",
            "[307 | 766.44] loss=1.57 avg=2.12\n",
            "[308 | 768.79] loss=1.48 avg=2.12\n",
            "[309 | 771.14] loss=1.33 avg=2.11\n",
            "[310 | 773.49] loss=1.88 avg=2.10\n",
            "[311 | 775.84] loss=1.34 avg=2.10\n",
            "[312 | 778.20] loss=1.18 avg=2.09\n",
            "[313 | 780.55] loss=1.26 avg=2.08\n",
            "[314 | 782.92] loss=1.61 avg=2.07\n",
            "[315 | 785.27] loss=1.67 avg=2.07\n",
            "[316 | 787.64] loss=1.50 avg=2.06\n",
            "[317 | 790.00] loss=1.36 avg=2.06\n",
            "[318 | 792.36] loss=1.80 avg=2.05\n",
            "[319 | 794.72] loss=1.51 avg=2.05\n",
            "[320 | 797.08] loss=1.28 avg=2.04\n",
            "[321 | 799.46] loss=1.84 avg=2.04\n",
            "[322 | 801.82] loss=1.43 avg=2.03\n",
            "[323 | 804.18] loss=2.09 avg=2.03\n",
            "[324 | 806.55] loss=1.64 avg=2.03\n",
            "[325 | 808.93] loss=1.91 avg=2.03\n",
            "[326 | 811.29] loss=1.50 avg=2.02\n",
            "[327 | 813.65] loss=1.41 avg=2.01\n",
            "[328 | 816.02] loss=1.45 avg=2.01\n",
            "[329 | 818.38] loss=1.08 avg=2.00\n",
            "[330 | 820.75] loss=1.59 avg=2.00\n",
            "[331 | 823.11] loss=1.65 avg=1.99\n",
            "[332 | 825.47] loss=1.47 avg=1.99\n",
            "[333 | 827.83] loss=1.15 avg=1.98\n",
            "[334 | 830.19] loss=1.35 avg=1.97\n",
            "[335 | 832.55] loss=1.92 avg=1.97\n",
            "[336 | 834.91] loss=1.63 avg=1.97\n",
            "[337 | 837.26] loss=1.24 avg=1.96\n",
            "[338 | 839.63] loss=1.08 avg=1.95\n",
            "[339 | 841.99] loss=1.89 avg=1.95\n",
            "[340 | 844.36] loss=1.47 avg=1.94\n",
            "[341 | 846.72] loss=1.02 avg=1.94\n",
            "[342 | 849.09] loss=1.19 avg=1.93\n",
            "[343 | 851.44] loss=1.26 avg=1.92\n",
            "[344 | 853.81] loss=1.22 avg=1.91\n",
            "[345 | 856.18] loss=1.13 avg=1.91\n",
            "[346 | 858.53] loss=1.45 avg=1.90\n",
            "[347 | 860.89] loss=1.50 avg=1.90\n",
            "[348 | 863.25] loss=1.19 avg=1.89\n",
            "[349 | 865.61] loss=1.35 avg=1.88\n",
            "[350 | 867.98] loss=1.37 avg=1.88\n",
            "[351 | 870.34] loss=1.41 avg=1.87\n",
            "[352 | 872.70] loss=1.26 avg=1.87\n",
            "[353 | 875.06] loss=1.19 avg=1.86\n",
            "[354 | 877.43] loss=1.27 avg=1.85\n",
            "[355 | 879.80] loss=1.44 avg=1.85\n",
            "[356 | 882.16] loss=1.43 avg=1.85\n",
            "[357 | 884.52] loss=1.58 avg=1.84\n",
            "[358 | 886.89] loss=1.35 avg=1.84\n",
            "[359 | 889.24] loss=1.27 avg=1.83\n",
            "[360 | 891.59] loss=1.07 avg=1.82\n",
            "[361 | 893.94] loss=1.51 avg=1.82\n",
            "[362 | 896.29] loss=1.48 avg=1.82\n",
            "[363 | 898.64] loss=1.25 avg=1.81\n",
            "[364 | 901.01] loss=1.25 avg=1.81\n",
            "[365 | 903.38] loss=0.97 avg=1.80\n",
            "[366 | 905.72] loss=1.36 avg=1.79\n",
            "[367 | 908.09] loss=0.92 avg=1.78\n",
            "[368 | 910.44] loss=1.25 avg=1.78\n",
            "[369 | 912.79] loss=1.28 avg=1.77\n",
            "[370 | 915.14] loss=1.45 avg=1.77\n",
            "[371 | 917.49] loss=1.45 avg=1.77\n",
            "[372 | 919.85] loss=1.00 avg=1.76\n",
            "[373 | 922.19] loss=1.39 avg=1.75\n",
            "[374 | 924.54] loss=0.96 avg=1.75\n",
            "[375 | 926.90] loss=1.04 avg=1.74\n",
            "[376 | 929.26] loss=1.29 avg=1.73\n",
            "[377 | 931.62] loss=0.93 avg=1.73\n",
            "[378 | 933.97] loss=1.05 avg=1.72\n",
            "[379 | 936.32] loss=0.95 avg=1.71\n",
            "[380 | 938.68] loss=0.89 avg=1.70\n",
            "[381 | 941.02] loss=1.24 avg=1.70\n",
            "[382 | 943.37] loss=1.32 avg=1.69\n",
            "[383 | 945.72] loss=1.05 avg=1.69\n",
            "[384 | 948.07] loss=0.98 avg=1.68\n",
            "[385 | 950.42] loss=1.05 avg=1.67\n",
            "[386 | 952.78] loss=0.94 avg=1.67\n",
            "[387 | 955.13] loss=1.14 avg=1.66\n",
            "[388 | 957.47] loss=1.42 avg=1.66\n",
            "[389 | 959.83] loss=1.09 avg=1.65\n",
            "[390 | 962.18] loss=1.27 avg=1.65\n",
            "[391 | 964.53] loss=1.14 avg=1.64\n",
            "[392 | 966.88] loss=1.04 avg=1.64\n",
            "[393 | 969.23] loss=1.15 avg=1.63\n",
            "[394 | 971.57] loss=0.84 avg=1.62\n",
            "[395 | 973.93] loss=0.95 avg=1.62\n",
            "[396 | 976.28] loss=0.81 avg=1.61\n",
            "[397 | 978.63] loss=0.91 avg=1.60\n",
            "[398 | 980.99] loss=1.53 avg=1.60\n",
            "[399 | 983.33] loss=1.01 avg=1.60\n",
            "[400 | 985.68] loss=1.10 avg=1.59\n",
            "======== SAMPLE 1 ========\n",
            " do. This was not for me.\"\n",
            "\n",
            "\"I have lived with you, Charles Darnay.\"\n",
            "\n",
            "\"I've lived with you all my life.\"\n",
            "\n",
            "\"I live near you, now. You must live with me somewhere. I\n",
            "shall never be a stranger to a place where I don't have to live.\"\n",
            "\n",
            "He was so far off, that her father, still bearing his old look, and still bearing\n",
            "his fatherly looks, both counted with him and against the Sheep of the House:\n",
            "he would beat his little compatriot on the knuckles (though, of course, he counted\n",
            "himself as somebody with a little going for him), but it was not worth that\n",
            "addition of trouble.\n",
            "\n",
            "\"But you have not lived with me for forty years.\"\n",
            "\n",
            "\"No.\"\n",
            "\n",
            "\"--And by 'twenty years' I mean your life--your wife's life, if anything\n",
            "necessary.\"\n",
            "\n",
            "\"No.\"\n",
            "\n",
            "His daughter was as yet entirely in the matter of his daughter, and as such he\n",
            "set her eyes upon her father.\n",
            "\n",
            "\"The Doctor. Is it not remarkable to see how much the stranger, the\n",
            "oppressor, is at variance with you?\"\n",
            "\n",
            "\"Not--not.\"\n",
            "\n",
            "\"--But it is not unreasonable for one who has nothing to fear, to say how\n",
            "uncompelling it is that--I wonder!--the miserable times--the times when I\n",
            "shall never see my father again!\"\n",
            "\n",
            "She could not speak to that; but, at the very thought of it in action, she\n",
            "felt it necessary to answer, \"Faw--faw!\"\n",
            "\n",
            "\"_I_ have--have suffered for forty years, from a Time-Turner, to die\n",
            "frightfully, but my daughter, through no malice or accident, has come to be\n",
            "my friend.\"\n",
            "\n",
            "The Doctor replied, with a smile:\n",
            "\n",
            "\"I have had sorrows of my own, dear, unhappy, painful, all. _I_ have\n",
            "suffered, my grief, not one bereavement in sight. There is a time when the\n",
            "vain affections and thoughts of all is turned towards one another, and that\n",
            "time is long dead.\"\n",
            "\n",
            "\"I hear you,\" said Doctor Manette, shaking his head, \"a blight on us all, dear\n",
            "child.\"\n",
            "\n",
            "\"I hear you, child. The blight is not so bad as you seem to think it is. The\n",
            "bitterness of it is, is (as I have often supposed) a change, a painful thing, and\n",
            "hard to account for. Hard to explain to a child, a Time-Turner user, who does\n",
            "certainly not understand the feeling. Harder to raise one's voice and career,\n",
            "on one's own account, in that difficult new and unhappy way, with\n",
            "no blind spot set on it, and on what I understand is both father and\n",
            "daughter's responsibility to bring about that change.\"\n",
            "\n",
            "\"I am glad you have come, child.\"\n",
            "\n",
            "\"But it is not my time to come? I did speak, child. I am not married?\"\n",
            "\n",
            "\"_I_ have no husband?\"\n",
            "\n",
            "\"But it may be worth saying, child, a little while ago, that I have no husband.\"\n",
            "\n",
            "\"You have no occasion to begrudge me your old time, child? When I married your\n",
            "father, I never once thought of coming to England, but I have looked round and\n",
            "nowsee--\"\n",
            "\n",
            "\"Yes, you see, there are so many of us there, child, and you think, 'what\n",
            "wouldn't it be most agreeable to have your home abroad?' You see, there is nothing else\n",
            "to see here, no land, no sea to sail upon? Now, even if you had, you cannot think of\n",
            "what you would have wanted to ask permission to have. You ask no stupid question, and nobody\n",
            "will tell you so.\"\n",
            "\n",
            "\"What would you have wanted?\"\n",
            "\n",
            "\"You would have asked no stupid question and nobody would have told you\n",
            "why you should have asked it. No wonder then, my child, that a summer day was\n",
            "put up for the asking of that question, and that a gentleman was brought away\n",
            "consequently awry, inasmuch as one of its own senses (possessed\n",
            "or not possessed) had failed to detect the waste of the World, and\n",
            "(perhaps also deliberately or accidentally) failed to detect the waste\n",
            "of the House.\"\n",
            "\n",
            "Charles Darnay looked a little startled, but still held the Doctor's hand.\n",
            "\"If you understand the gentleman as saying something to you, you would have\n",
            "spoken to Charles, you know. But you would have not. Is there something that needs asking?\"\n",
            "\n",
            "\"It is very hard to ask the House. You may ask them\n",
            "\n",
            "[401 | 1000.17] loss=1.11 avg=1.59\n",
            "[402 | 1002.51] loss=1.29 avg=1.58\n",
            "[403 | 1004.86] loss=0.74 avg=1.57\n",
            "[404 | 1007.23] loss=1.15 avg=1.57\n",
            "[405 | 1009.58] loss=1.13 avg=1.57\n",
            "[406 | 1011.95] loss=1.05 avg=1.56\n",
            "[407 | 1014.32] loss=0.89 avg=1.55\n",
            "[408 | 1016.69] loss=0.92 avg=1.55\n",
            "[409 | 1019.05] loss=0.65 avg=1.54\n",
            "[410 | 1021.43] loss=0.95 avg=1.53\n",
            "[411 | 1023.80] loss=0.96 avg=1.53\n",
            "[412 | 1026.17] loss=1.01 avg=1.52\n",
            "[413 | 1028.53] loss=1.14 avg=1.52\n",
            "[414 | 1030.91] loss=0.83 avg=1.51\n",
            "[415 | 1033.27] loss=0.73 avg=1.50\n",
            "[416 | 1035.64] loss=0.92 avg=1.50\n",
            "[417 | 1038.01] loss=0.89 avg=1.49\n",
            "[418 | 1040.37] loss=0.90 avg=1.48\n",
            "[419 | 1042.73] loss=1.04 avg=1.48\n",
            "[420 | 1045.10] loss=0.84 avg=1.47\n",
            "[421 | 1047.45] loss=1.27 avg=1.47\n",
            "[422 | 1049.82] loss=0.80 avg=1.46\n",
            "[423 | 1052.18] loss=1.00 avg=1.46\n",
            "[424 | 1054.54] loss=1.03 avg=1.46\n",
            "[425 | 1056.90] loss=1.11 avg=1.45\n",
            "[426 | 1059.25] loss=0.74 avg=1.44\n",
            "[427 | 1061.60] loss=0.89 avg=1.44\n",
            "[428 | 1063.97] loss=0.70 avg=1.43\n",
            "[429 | 1066.32] loss=0.89 avg=1.43\n",
            "[430 | 1068.69] loss=0.82 avg=1.42\n",
            "[431 | 1071.06] loss=0.55 avg=1.41\n",
            "[432 | 1073.43] loss=0.86 avg=1.41\n",
            "[433 | 1075.78] loss=0.75 avg=1.40\n",
            "[434 | 1078.16] loss=0.74 avg=1.39\n",
            "[435 | 1080.53] loss=0.87 avg=1.39\n",
            "[436 | 1082.88] loss=0.88 avg=1.38\n",
            "[437 | 1085.25] loss=1.39 avg=1.38\n",
            "[438 | 1087.61] loss=0.73 avg=1.38\n",
            "[439 | 1089.98] loss=0.55 avg=1.37\n",
            "[440 | 1092.36] loss=1.10 avg=1.36\n",
            "[441 | 1094.71] loss=0.70 avg=1.36\n",
            "[442 | 1097.07] loss=0.58 avg=1.35\n",
            "[443 | 1099.43] loss=0.98 avg=1.35\n",
            "[444 | 1101.80] loss=0.87 avg=1.34\n",
            "[445 | 1104.16] loss=0.76 avg=1.34\n",
            "[446 | 1106.52] loss=0.58 avg=1.33\n",
            "[447 | 1108.88] loss=0.52 avg=1.32\n",
            "[448 | 1111.24] loss=0.61 avg=1.31\n",
            "[449 | 1113.60] loss=0.72 avg=1.31\n",
            "[450 | 1115.96] loss=0.64 avg=1.30\n",
            "[451 | 1118.32] loss=0.61 avg=1.29\n",
            "[452 | 1120.68] loss=0.78 avg=1.29\n",
            "[453 | 1123.03] loss=0.72 avg=1.28\n",
            "[454 | 1125.38] loss=0.60 avg=1.27\n",
            "[455 | 1127.75] loss=0.86 avg=1.27\n",
            "[456 | 1130.10] loss=0.53 avg=1.26\n",
            "[457 | 1132.45] loss=0.72 avg=1.26\n",
            "[458 | 1134.80] loss=0.65 avg=1.25\n",
            "[459 | 1137.17] loss=0.74 avg=1.25\n",
            "[460 | 1139.53] loss=1.03 avg=1.24\n",
            "[461 | 1141.88] loss=0.78 avg=1.24\n",
            "[462 | 1144.23] loss=0.70 avg=1.23\n",
            "[463 | 1146.58] loss=0.64 avg=1.23\n",
            "[464 | 1148.96] loss=0.51 avg=1.22\n",
            "[465 | 1151.31] loss=0.44 avg=1.21\n",
            "[466 | 1153.67] loss=0.55 avg=1.21\n",
            "[467 | 1156.03] loss=0.60 avg=1.20\n",
            "[468 | 1158.39] loss=0.63 avg=1.19\n",
            "[469 | 1160.75] loss=0.63 avg=1.19\n",
            "[470 | 1163.10] loss=0.76 avg=1.18\n",
            "[471 | 1165.46] loss=0.79 avg=1.18\n",
            "[472 | 1167.83] loss=0.81 avg=1.18\n",
            "[473 | 1170.19] loss=0.45 avg=1.17\n",
            "[474 | 1172.55] loss=1.02 avg=1.17\n",
            "[475 | 1174.92] loss=0.66 avg=1.16\n",
            "[476 | 1177.29] loss=0.89 avg=1.16\n",
            "[477 | 1179.65] loss=0.97 avg=1.16\n",
            "[478 | 1182.01] loss=0.53 avg=1.15\n",
            "[479 | 1184.37] loss=0.61 avg=1.15\n",
            "[480 | 1186.73] loss=0.68 avg=1.14\n",
            "[481 | 1189.09] loss=0.62 avg=1.14\n",
            "[482 | 1191.46] loss=0.65 avg=1.13\n",
            "[483 | 1193.82] loss=0.87 avg=1.13\n",
            "[484 | 1196.18] loss=0.46 avg=1.12\n",
            "[485 | 1198.54] loss=0.68 avg=1.12\n",
            "[486 | 1200.92] loss=0.55 avg=1.11\n",
            "[487 | 1203.27] loss=1.26 avg=1.11\n",
            "[488 | 1205.63] loss=0.51 avg=1.11\n",
            "[489 | 1207.97] loss=0.43 avg=1.10\n",
            "[490 | 1210.33] loss=0.67 avg=1.10\n",
            "[491 | 1212.68] loss=0.47 avg=1.09\n",
            "[492 | 1215.03] loss=0.57 avg=1.08\n",
            "[493 | 1217.40] loss=0.35 avg=1.08\n",
            "[494 | 1219.75] loss=0.61 avg=1.07\n",
            "[495 | 1222.10] loss=0.55 avg=1.07\n",
            "[496 | 1224.47] loss=0.68 avg=1.06\n",
            "[497 | 1226.82] loss=0.63 avg=1.06\n",
            "[498 | 1229.17] loss=0.66 avg=1.05\n",
            "[499 | 1231.53] loss=0.56 avg=1.05\n",
            "[500 | 1233.90] loss=0.38 avg=1.04\n",
            "Saving checkpoint/run1/model-500\n",
            "\"It is a pity, look you at my work. Look you at my eyes. I have eyes for drawing pictures. How do I--\"\n",
            "\n",
            "\"Shall I ask you, Mr. Cruncher?\"\n",
            "\n",
            "\"Surely, Mr. Darnay.\"\n",
            "\n",
            "\"Surely, Mr. Darnay!\"\n",
            "\n",
            "\"Surely, Mr. Cruncher!\"\n",
            "\n",
            "\"No thanks to you for your admiration of my innermost self; it is all yours. If I may\n",
            "chair-bound go up and down, in letters as usual, in any ordinary walk, talk,\n",
            "emancement, correspondence, talk, life--don't speak a word of this here; it is a\n",
            "letter.\"\n",
            "\n",
            "\"And why not,\" said Darnay, with a careless wave of his hand, rather\n",
            "than a peal of the Doctor's: \"I have a feeling that you will believe\n",
            "in a great deal of the nonsense of this Court.\"\n",
            "\n",
            "\"I don't doubt it,\" said Mr. Cruncher, with a musing laugh. \"I have been pondering\n",
            "what to say to you, and I think--I once said to a young lady walking\n",
            "down the streets, with her hands in his pockets, that if he really did\n",
            "want gold, he wouldn't be long staring at it, unless he really did\n",
            "want it. If he really did want that, he'd have it at some great\n",
            "greationed place, far out in the world.\"\n",
            "\n",
            "\"Ay! to be prayed out of the matter,\" said Mr. Cruncher, with an air of\n",
            "discovery. \"If he really did want that, he'd have it in a chest\n",
            "near him on the table, somewhere, on his reading-table. Give\n",
            "that chest to him, Mr. Darnay!\"\n",
            "\n",
            "There was a rush in the building, and Darnay didn't look like he\n",
            "would've rushed out with anything more than a coat and grey\n",
            "shoes, or a grey hat and black cockade. His black cockade was a\n",
            "little too much for his pale frame, and his short legs got in the way; his\n",
            "haggard frame, too, reflected his rough expression, and showed the rough\n",
            "conception that there was something amiss. He stopped short, and took a stool\n",
            "out as he went; not trusting the cold and security of the old\n",
            "chair, which might have got him in some wet scrapes.\n",
            "\n",
            "\"You are a little late, miss, or I might need an escort. Tell Lucie\n",
            "and I, and I'll have a short talk with you.\"\n",
            "\n",
            "\"I am going to tell her.\"\n",
            "\n",
            "\"By appointment, miss.\"\n",
            "\n",
            "\"Oh, by appointment!\"\n",
            "\n",
            "\"So, by appointment. I'll have a short talk with you, either\n",
            "in the sitting-room by the fountain, or by the garret till it is\n",
            "sumptuous, or evensaddle at the stone fountain.\"\n",
            "\n",
            "\"I shall have a short talk with you, Mr. Darnay!\"\n",
            "\n",
            "\"An instant, Mr. Darnay! Mr. Darnay, when you get home, don't\n",
            "t`rum-- don't t'wind me a word as I go, or as you go on. I wish you\n",
            "hadn't got my way; don't pull me down like that. I wish you had a\n",
            "handsome heart. I wish you had a heartful and thankful soul. I\n",
            "hope your hair is a shining example of a gentle soul; a\n",
            "gentleman like you, who has touched so dear a fellow, a gentle fellow\n",
            "like yourself, a gentle fellow worthy of yours; a gentle fellow who\n",
            "has touched you all, a gentle fellow worthy of mine. I wish to have that\n",
            "handsome heart, no matter what I may do. I wish to know how you\n",
            "reared up, and where you are going; I wish to know whether you are\n",
            "coming from; I wish to know if you speak French well, or how you speak\n",
            "French at all. And most of all, I wish to know whether you speak, sing,\n",
            "or read French.\"\n",
            "\n",
            "\"Hark!\" said the Doctor of Beauvais.\n",
            "\n",
            "\"Hark!\" returned the Doctor of Beauvais, still holding his hand, in a\n",
            "resolute and uncompromising manner.\n",
            "\n",
            "\"I am not distracted; I shall not be distracted. I shall not interfere. I\n",
            "shall do what I can to prevent your timely return; I hope you\n",
            "are not in any danger.\"\n",
            "\n",
            "\"Doctor Manette, I am very sorry; I am very sorry. I am not in any danger. I have\n",
            "never been in danger of dying of an overdose of any narcotic. I\n",
            "have had no occasion to die. I beg you to heed my\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7abzJJNHUT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}