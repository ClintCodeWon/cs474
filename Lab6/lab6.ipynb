{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YpKBiuiVdFs",
        "colab_type": "text"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7bdZWxvJrsx",
        "outputId": "d55457e6-45cf-4d5c-fdde-f3bdb00cc4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-14 17:03:48--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.4.91.75, 34.192.194.48, 34.237.183.157, ...\n",
            "Connecting to piazza.com (piazza.com)|52.4.91.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2020-02-14 17:03:48--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 13.35.112.107, 13.35.112.59, 13.35.112.222, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|13.35.112.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-02-14 17:03:48 (10.0 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "file_len = 2579888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxBeKeNjJ0NQ",
        "outputId": "a9ec711c-ebda-47e0-f397-36bc1811f02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n the top they found, as Strider had said, a wide ring of ancient \n",
            "stonework, now crumbling or covered with age-long grass. But in the centre a \n",
            "cairn of broken stones had been piled. They were blacken\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On0_WitWJ99e",
        "outputId": "efa12b5f-0175-4a3f-f5df-2873bc0049aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavAv50ZKQ-F",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.W_ir = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hr = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W_iz = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hz = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W_in = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hn = nn.Linear(hidden_size, hidden_size)\n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    r_t = torch.nn.functional.sigmoid(self.W_ir(inputs) + self.W_hr(hidden))\n",
        "    z_t = torch.nn.functional.sigmoid(self.W_iz(inputs) + self.W_hz(hidden))\n",
        "    n_t = torch.nn.functional.tanh(self.W_in(inputs) + (r_t*(self.W_hn(hidden))))\n",
        "    hidden = ((1 - z_t)*n_t) + (z_t*hidden)\n",
        "    output = hidden\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    \n",
        "    return output, hidden\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6tNdEnzWj5F",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    # self.gru = GRU(hidden_size, hidden_size, n_layers)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    # more stuff here...\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    # stuff here\n",
        "    # pdb.set_trace()\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    #pdb.set_trace()\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.out(output)\n",
        "    return output, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhXghEPKD-5",
        "colab": {}
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ALC3Pf8Kbsi",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "\n",
        "  #pdb.set_trace()\n",
        "  for di in range(inp.size(0)):\n",
        "    output, hidden = decoder(inp[di], hidden)\n",
        "    # pdb.set_trace()\n",
        "    myTarget = target[di].unsqueeze(0)\n",
        "    myOutput = output.squeeze(0)\n",
        "    # pdb.set_trace()\n",
        "    loss += criterion(myOutput, myTarget)\n",
        "\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / inp.size(0)\n",
        "\n",
        "    \n",
        "  # more stuff here..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-bp-OZ1KjNh",
        "colab": {}
      },
      "source": [
        "def sample_outputs(output, temp):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    T = temp\n",
        "    return torch.multinomial(torch.exp(output) / T, 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  ans = \"\"\n",
        "  with torch.no_grad():\n",
        "    hidden = decoder.init_hidden()\n",
        "    ans += prime_str\n",
        "\n",
        "    for char in prime_str:\n",
        "      output, hidden = decoder(char_tensor(char), hidden)\n",
        "\n",
        "    for i in range(predict_len):\n",
        "      output, hidden = decoder(char_tensor(ans[-1]), hidden)\n",
        "      next_char = all_characters[sample_outputs(output.squeeze(0), temperature)]\n",
        "      ans += next_char\n",
        "\n",
        "  return ans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nXFeCmdKodw",
        "colab": {}
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 500\n",
        "plot_every = 100\n",
        "hidden_size = 200\n",
        "n_layers = 1\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "inp, target = random_training_set()\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKfozqw-6eqb",
        "outputId": "2459ee71-db79-4021-f92d-525430835568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[160.0490756034851 (500 10%) 2.1265]\n",
            "Why. \n",
            "\n",
            "'Af on \n",
            "hady bis the marnted the qouly fo thele roce fordire the brincly bcougs the bear piderd \n",
            "\n",
            "[315.1375916004181 (1000 20%) 1.7720]\n",
            "Wher been sevend beach wour Plagh Sarwal. But betras!' Sam \n",
            "him, I drond, and agoned brayderid \n",
            "goomby \n",
            "\n",
            "[470.18575286865234 (1500 30%) 1.6771]\n",
            "Whed or \n",
            "coult, yet was fell a \n",
            "not firetherth, It willoming frodo'le werey; and mind!' he wearing Sal \n",
            "\n",
            "[624.9832427501678 (2000 40%) 1.6129]\n",
            "Whilly \n",
            "off the Strunk \n",
            "benoth an - thre ca. \n",
            "\n",
            "'Not with he said,' said AromoFs of I knove and been al \n",
            "\n",
            "[780.3138167858124 (2500 50%) 1.7916]\n",
            "Whel,' said Sam, \n",
            "slven only all Frodo he \n",
            "am in the hidge \n",
            "tye,; agains endalf of your frum of mide.' \n",
            "\n",
            "[934.9903318881989 (3000 60%) 1.4160]\n",
            "Wh at them se maskned feltient I but hanging to to say took for him,' said Ly he have see nigst. The t \n",
            "\n",
            "[1090.3390100002289 (3500 70%) 1.7008]\n",
            "Whelly, river of If was brough the dark and all. \n",
            "\n",
            "It ham dogevay the Men leam the know. I kon were yo \n",
            "\n",
            "[1244.9345769882202 (4000 80%) 1.5347]\n",
            "Whose crumber do the \n",
            "chairing, fyer a coupless, or \n",
            "not of \n",
            "me the sungress, must center has every, ' \n",
            "\n",
            "[1400.4253616333008 (4500 90%) 1.5281]\n",
            "Wheron, fell or ever many \n",
            "a follow after a slift!' he searly them: my pass,' said that way, and stopp \n",
            "\n",
            "[1554.5766780376434 (5000 100%) 1.5047]\n",
            "Wh oremanifint of the good with Denared flet deep day byath \n",
            "very gobing sop it was enves \n",
            "it have, an \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ee0so6aKJ5L8",
        "outputId": "1f204c61-e56e-4301-af05-3ed27946bf6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " he\n",
            " hed and keep about finely as I'm clan-sweet \n",
            "to the king, \n",
            "year trusted with the Fite. Not alsifold- and there nouthold \n",
            "vises he could deep tur and went of roavest night to he. \n",
            "\n",
            "Slat beyond in from th \n",
            "\n",
            " I \n",
            " I on othent \n",
            "twell, \n",
            "eybelged down dievented that he fend older their care for \n",
            "out, I don't did never you was pangrent hard. \n",
            "\n",
            "'Never \n",
            "any sing Placoin was ranging upon the Sensat. They shall now steng \n",
            "\n",
            " I \n",
            " I would say, should me how even with his \n",
            "\n",
            "\n",
            "\n",
            "walkness dour!' Gandalf on Pippin those f(S, in a \n",
            "many namelt; and have no for age before the purkee of \n",
            "then redious \n",
            "Nartting was lanst entrish. Therery \n",
            " \n",
            "\n",
            " he\n",
            " hed, as we \n",
            "lang Lord on a rim \n",
            "it have deep darkness he wonder; Gondot thus he sleef the City, though Shjoded that he nerry he ramselfached uncest the Smove Hord have been wath faight of the \n",
            "\n",
            "\n",
            "\n",
            "glever \n",
            "\n",
            " I \n",
            " I mon on fine to exile stase, \n",
            "to hand was son't blance \n",
            "\n",
            "which \n",
            "though \n",
            "their men the Woomer have am our wark will of Buridly. 'Aftrying Past the bound. The mouth for it stood that wathing. Not dou in  \n",
            "\n",
            " Th\n",
            " They wentt to the suhnoubhing shadow the \n",
            "Ring stast: he was down even \n",
            "\n",
            "\n",
            "gate at \n",
            "seemed rishe of a whill. 'Gand,' what it \n",
            "feek up! Tell \n",
            "\n",
            "should had back heabing on us an old. Evgrass to cante not li \n",
            "\n",
            " G\n",
            " Gomber and stailing on him. Not tope one the fit the awair the sunly. \n",
            "\n",
            "The dranged present,' an our comvain answered, sitting a pack. 'The unhall for \n",
            "deeping gone of the \n",
            "put that the lands to the we \n",
            "\n",
            " ca\n",
            " cage Gandalf of the Load soon of come them doement, his before their fear, \n",
            "Splant upon his sug. \n",
            "\n",
            "And tenter will war have be your dangers whine harde must and least on the moon.' \n",
            "\n",
            "'Bust and quits on  \n",
            "\n",
            " ca\n",
            " cage walk. \n",
            "\n",
            "\n",
            "\n",
            "Don't he seemed his way from the upited, as why we know; retoot a clountal wanfully and very passed to roots and tack \n",
            "out; Forgown as such of brew. \n",
            "\n",
            "He leaves \n",
            "cowing,' he said. How and \n",
            "\n",
            " I \n",
            " I ore, and apprangy; for the bey wormy back oreming \n",
            "and \n",
            "the ofte was \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Place howed and went of my, \n",
            "or come long of fear and the percles coulden back.' \n",
            "\n",
            "'His at Lought of the horn. \n",
            "\n",
            "'On \n",
            "they ror \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHSnox9TCvm",
        "colab_type": "code",
        "outputId": "71fd93ff-fafa-47dc-e97d-9e7809ea7019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f08f152ca90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j+47DviKlRI8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkqxGoZYBeA1",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNSKpbv27nl",
        "colab_type": "code",
        "outputId": "d35a2e09-0758-44f0-c344-c1c5596be77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "print(random_chunk())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 1115394\n",
            "ad still wear the crown,\n",
            "I here resign my government to thee,\n",
            "For thou art fortunate in all thy deeds.\n",
            "\n",
            "WARWICK:\n",
            "Your grace hath still been famed for virtuous;\n",
            "And now may seem as wise as virtuous,\n",
            "By \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6Ty18w2_uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 5000\n",
        "print_every = 500\n",
        "plot_every = 100\n",
        "hidden_size = 200\n",
        "n_layers = 1\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "inp, target = random_training_set()\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVNmLipfu7Ny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8233fbfa-d776-493a-a665-331b0e1586f6"
      },
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[157.45819664001465 (500 10%) 2.0287]\n",
            "Why mant,\n",
            "Why legeme I if ms the turngst Eyst fain save,\n",
            "Wo france foancaried?\n",
            "Well, unk, I fanten Hap \n",
            "\n",
            "[309.2087688446045 (1000 20%) 1.8170]\n",
            "Whorstrane\n",
            "she ford wimant mawst with you comen:\n",
            "I way I comcoigh\n",
            "lobhile of Rinows, I pare his sweart \n",
            "\n",
            "[460.45297741889954 (1500 30%) 1.7173]\n",
            "Wherverd:\n",
            "To prichil:\n",
            "And comporing own dagile, Call deviton fries.\n",
            "\n",
            "AmTIENNUS:\n",
            "Boney nor resid it Nus \n",
            "\n",
            "[611.9716420173645 (2000 40%) 1.7874]\n",
            "Wharr.\n",
            "\n",
            "MERGAUCESTENC:\n",
            "Manish\n",
            "A do reme, son, though hath begwfand excuris.\n",
            "\n",
            "Hearr that most sught;\n",
            "Ih \n",
            "\n",
            "[764.0012674331665 (2500 50%) 1.5012]\n",
            "Whird, Semb'd roubtle must, say'd bid paren's grean the world;\n",
            "For this no you the gorns, and make hen \n",
            "\n",
            "[914.9174721240997 (3000 60%) 1.9378]\n",
            "Whirs: homing gracial.\n",
            "\n",
            "LUCEONE:\n",
            "He have her myself o him stridged\n",
            "And a partated one you not has thou \n",
            "\n",
            "[1066.4146196842194 (3500 70%) 1.4967]\n",
            "Whenced,' of fine!\n",
            "On? gave, and we will to-no murd, cannot with of Clempery.\n",
            "\n",
            "GiLABETH:\n",
            "Why, whore sh \n",
            "\n",
            "[1217.6201691627502 (4000 80%) 1.5868]\n",
            "Who: is: I thanksh'd\n",
            "Unneless ters, fight, since; and soot fails.\n",
            "\n",
            "WARWICK:\n",
            "Sweet will fear.\n",
            "\n",
            "Second S \n",
            "\n",
            "[1371.0280003547668 (4500 90%) 1.9093]\n",
            "Whine\n",
            "Kspace to he bear druck to-band's insurs, well,\n",
            "When him told thee to.\n",
            "Thee deign Lind RING HENR \n",
            "\n",
            "[1521.6038665771484 (5000 100%) 1.7928]\n",
            "Whal'd say they three\n",
            "Time me world.\n",
            "\n",
            "ESCAPULEL:\n",
            "How conseeviels decteen to ggoble own:\n",
            "He, dishouson  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pK20oudvEAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a13485fe-36d6-455f-cd71-b018246ae394"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " wh\n",
            " whaft the right,\n",
            "Look Romarch her redures,\n",
            "And conselled comfort, man!\n",
            "\n",
            "QUEEN WICHARD III:\n",
            "IV courth, to a great lefe,\n",
            "The king head, DiEkers' right, by the noble.\n",
            "\n",
            "ELIO:\n",
            "With worl'd, and me him, Peliev \n",
            "\n",
            " ca\n",
            " cal when her it:\n",
            "Come, your man dream,\n",
            "If hear me, witwhe fell unto the would great butchio.\n",
            "Fathem; and a lones fiesal vost kite pardon\n",
            "but at and bury, of they slay take and with me the mejco.\n",
            "\n",
            "HASCES \n",
            "\n",
            " wh\n",
            " whoenger my father some firged\n",
            "The Tibgeen enurn the bark, what\n",
            "Heven at an her acion'sw awail-villain mistre\n",
            "As Edwards I day, and thaunking from by sir?\n",
            "\n",
            "MERCUTI:\n",
            "The eittle hence: ask you'rownering;\n",
            " \n",
            "\n",
            " ca\n",
            " cate and brother.\n",
            "\n",
            "BOLUSSTER:\n",
            "Now, go? I have on your hunds too canst,\n",
            "Your grows instance the cere!\n",
            "\n",
            "TAUCLAGE:\n",
            "Till of me, inisher's now tempera that hearth;\n",
            "The prooutter,--\n",
            "And for a touch in the dot \n",
            "\n",
            " ra\n",
            " ral the poly atter-butch'd him noble countor.\n",
            "Soval nogk, but dame! him hear them die;\n",
            "Who Edwark burb it is gentle noble of Amath\n",
            "Thou let need when dufter strike dease of hath my crown?\n",
            "That leath.\n",
            "\n",
            "D \n",
            "\n",
            " Th\n",
            " Tho thank your are me;\n",
            "Eo. But name and so in askdred underful.\n",
            "\n",
            "LUCESTER:\n",
            "The friend her fell of ssolled so\n",
            "With rapry,\n",
            "The remeech'd so must now the couson,\n",
            "The sshapt against notration, go!\n",
            "I villuni \n",
            "\n",
            " lo\n",
            " lok more's dead thee;\n",
            "Thy Levien meet, to meage,--\n",
            "\n",
            "RUFLET:\n",
            "I'll fortune good the pretcut\n",
            "That stray your curbry, my, there holageds: therefore opfort?\n",
            "\n",
            "DUKE SARGHESS:\n",
            "So kings and leared the head when  \n",
            "\n",
            " I \n",
            " I to doiner'd to bolong,\n",
            "When your merread? I have you beoof!\n",
            "\n",
            "PETRUMH:\n",
            "Come, to my mothers.\n",
            "\n",
            "DUKEd:\n",
            "\n",
            "ESCALUS:\n",
            "No, myself and has.\n",
            "Think her hear napries send thousand forguing witch was ord-mind?\n",
            "\n",
            "TRAN \n",
            "\n",
            " G\n",
            " Gord:\n",
            "And, my mindel; whis may give. His he hamp a\n",
            "to heavings wife the bring such! in\n",
            "should hap Peaces on: he's norky me soul, and wele hands,\n",
            "Therefore, I tear by our great, whom forfore,\n",
            "Their comq \n",
            "\n",
            " ca\n",
            " cae of the bety meanorn:\n",
            "Whiccest Powar, Some camfort to you aresey, and that Plaute,\n",
            "Sir, petches' all such at.\n",
            "Lown all me we should be him!\n",
            "\n",
            "GLOUCESTER:\n",
            "Which dook want my to men thank her.\n",
            "\n",
            "MINGLY:\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFw-WiWxXAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}