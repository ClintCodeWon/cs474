{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_DcCrREoZnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    #ax2.xaxis.tick_top()\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    #ax2.xaxis.set_label_position('top')\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)\n",
        "\n",
        "class SkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(SkipEnv, self).__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            t_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer = []\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class PreProcessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(PreProcessFrame, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=(80,80,1), dtype=np.uint8)\n",
        "    def observation(self, obs):\n",
        "        return PreProcessFrame.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "\n",
        "        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n",
        "\n",
        "        new_frame = 0.299*new_frame[:,:,0] + 0.587*new_frame[:,:,1] + \\\n",
        "                    0.114*new_frame[:,:,2]\n",
        "\n",
        "        new_frame = new_frame[35:195:2, ::2].reshape(80,80,1)\n",
        "\n",
        "        return new_frame.astype(np.uint8)\n",
        "\n",
        "class MoveImgChannel(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(MoveImgChannel, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                            shape=(self.observation_space.shape[-1],\n",
        "                                   self.observation_space.shape[0],\n",
        "                                   self.observation_space.shape[1]),\n",
        "                            dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class ScaleFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
        "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
        "                             dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = SkipEnv(env)\n",
        "    env = PreProcessFrame(env)\n",
        "    env = MoveImgChannel(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaleFrame(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzwxSKSDMzwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, ALPHA):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        #self.conv1 = nn.Conv2d(3, 32, 8, stride=4, padding=1)\n",
        "        self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        #self.fc1 = nn.Linear(128*23*16, 512)\n",
        "        self.fc1 = nn.Linear(128*19*8, 512)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "        #self.optimizer = optim.SGD(self.parameters(), lr=self.ALPHA, momentum=0.9)\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=ALPHA)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        observation = T.Tensor(observation).to(self.device)\n",
        "        #observation = observation.view(-1, 3, 210, 160).to(self.device)\n",
        "        observation = observation.view(-1, 1, 185, 95)\n",
        "        observation = F.relu(self.conv1(observation))\n",
        "        observation = F.relu(self.conv2(observation))\n",
        "        observation = F.relu(self.conv3(observation))\n",
        "        #observation = observation.view(-1, 128*23*16).to(self.device)\n",
        "        observation = observation.view(-1, 128*19*8)\n",
        "        observation = F.relu(self.fc1(observation))\n",
        "        actions = self.fc2(observation)\n",
        "        return actions\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, gamma, epsilon, alpha,\n",
        "                 maxMemorySize, epsEnd=0.05,\n",
        "                 replace=10000, actionSpace=[0,1,2,3,4,5]):\n",
        "        self.GAMMA = gamma\n",
        "        self.EPSILON = epsilon\n",
        "        self.EPS_END = epsEnd\n",
        "        self.ALPHA = alpha\n",
        "        self.actionSpace = actionSpace\n",
        "        self.memSize = maxMemorySize\n",
        "        self.steps = 0\n",
        "        self.learn_step_counter = 0\n",
        "        self.memory = []\n",
        "        self.memCntr = 0\n",
        "        self.replace_target_cnt = replace\n",
        "        self.Q_eval = DeepQNetwork(alpha)\n",
        "        self.Q_next = DeepQNetwork(alpha)\n",
        "\n",
        "    def storeTransition(self, state, action, reward, state_):\n",
        "        if self.memCntr < self.memSize:\n",
        "            self.memory.append([state, action, reward, state_])\n",
        "        else:\n",
        "            self.memory[self.memCntr%self.memSize] = [state, action, reward, state_]\n",
        "        self.memCntr += 1\n",
        "\n",
        "    def chooseAction(self, observation):\n",
        "        rand = np.random.random()\n",
        "        actions = self.Q_eval.forward(observation)\n",
        "        if rand < 1 - self.EPSILON:\n",
        "            action = T.argmax(actions[1]).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.actionSpace)\n",
        "        self.steps += 1\n",
        "        return action\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        if self.replace_target_cnt is not None and \\\n",
        "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
        "\n",
        "        if self.memCntr+batch_size < self.memSize:\n",
        "            memStart = int(np.random.choice(range(self.memCntr)))\n",
        "        else:\n",
        "            memStart = int(np.random.choice(range(self.memSize-batch_size-1)))\n",
        "        miniBatch=self.memory[memStart:memStart+batch_size]\n",
        "        memory = np.array(miniBatch)\n",
        "\n",
        "        # convert to list because memory is an array of numpy objects\n",
        "        Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)\n",
        "        Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)\n",
        "\n",
        "        maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)\n",
        "        rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
        "        Qtarget = Qpred.clone()\n",
        "        indices = np.arange(batch_size)\n",
        "        Qtarget[indices,maxA] = rewards + self.GAMMA*T.max(Qnext[1])\n",
        "\n",
        "        if self.steps > 500:\n",
        "            if self.EPSILON - 1e-4 > self.EPS_END:\n",
        "                self.EPSILON -= 1e-4\n",
        "            else:\n",
        "                self.EPSILON = self.EPS_END\n",
        "\n",
        "        #Qpred.requires_grad_()\n",
        "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KClk4DKJjxN",
        "colab_type": "code",
        "outputId": "f4863634-dd92-4450-935f-4b8746ba0021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_MIm8-sUoXy",
        "colab_type": "code",
        "outputId": "8d2038cf-7542-4ad9-8580-d8c6ae9d4333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "import numpy as np\n",
        "from gym import wrappers\n",
        "\n",
        "env = gym.make('SpaceInvaders-v0')\n",
        "brain = Agent(gamma=0.95, epsilon=1.0, \n",
        "              alpha=0.003, maxMemorySize=5000,\n",
        "              replace=None)\n",
        "while brain.memCntr < brain.memSize:\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
        "        action = env.action_space.sample()\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        if done and info['ale.lives'] == 0:\n",
        "            reward = -100\n",
        "        brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                            np.mean(observation_[15:200,30:125], axis=2))\n",
        "        observation = observation_\n",
        "print('done initializing memory')\n",
        "\n",
        "scores = []\n",
        "epsHistory = []\n",
        "numGames = 50\n",
        "batch_size=32\n",
        "# uncomment the line below to record every episode.\n",
        "env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
        "for i in range(numGames):\n",
        "    print('starting game ', i+1, 'epsilon: %.4f' % brain.EPSILON)\n",
        "    epsHistory.append(brain.EPSILON)\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    frames = [np.sum(observation[15:200,30:125], axis=2)]\n",
        "    score = 0\n",
        "    lastAction = 0\n",
        "    while not done:\n",
        "        if len(frames) == 3:\n",
        "            action = brain.chooseAction(frames)\n",
        "            frames = []\n",
        "        else:\n",
        "            action = lastAction\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        frames.append(np.sum(observation_[15:200,30:125], axis=2))\n",
        "        if done and info['ale.lives'] == 0:\n",
        "            reward = -100\n",
        "        brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                              np.mean(observation_[15:200,30:125], axis=2))\n",
        "        observation = observation_\n",
        "        brain.learn(batch_size)\n",
        "        lastAction = action\n",
        "        #env.render(\n",
        "    scores.append(score)\n",
        "    print('score:',score)\n",
        "x = [i+1 for i in range(numGames)]\n",
        "fileName = str(numGames) + 'Games' + 'Gamma' + str(brain.GAMMA) + \\\n",
        "            'Alpha' + str(brain.ALPHA) + 'Memory' + str(brain.memSize)+ '.png'\n",
        "plotLearning(x, scores, epsHistory, fileName)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done initializing memory\n",
            "starting game  1 epsilon: 1.0000\n",
            "score: 125.0\n",
            "starting game  2 epsilon: 1.0000\n",
            "score: 95.0\n",
            "starting game  3 epsilon: 1.0000\n",
            "score: 105.0\n",
            "starting game  4 epsilon: 0.9658\n",
            "score: 60.0\n",
            "starting game  5 epsilon: 0.9074\n",
            "score: 210.0\n",
            "starting game  6 epsilon: 0.8059\n",
            "score: 220.0\n",
            "starting game  7 epsilon: 0.7185\n",
            "score: 155.0\n",
            "starting game  8 epsilon: 0.6525\n",
            "score: 145.0\n",
            "starting game  9 epsilon: 0.5881\n",
            "score: 40.0\n",
            "starting game  10 epsilon: 0.5497\n",
            "score: 120.0\n",
            "starting game  11 epsilon: 0.4720\n",
            "score: 330.0\n",
            "starting game  12 epsilon: 0.3758\n",
            "score: 180.0\n",
            "starting game  13 epsilon: 0.2929\n",
            "score: 210.0\n",
            "starting game  14 epsilon: 0.2075\n",
            "score: 180.0\n",
            "starting game  15 epsilon: 0.1235\n",
            "score: 210.0\n",
            "starting game  16 epsilon: 0.0500\n",
            "score: 545.0\n",
            "starting game  17 epsilon: 0.0500\n",
            "score: 210.0\n",
            "starting game  18 epsilon: 0.0500\n",
            "score: 700.0\n",
            "starting game  19 epsilon: 0.0500\n",
            "score: 505.0\n",
            "starting game  20 epsilon: 0.0500\n",
            "score: 200.0\n",
            "starting game  21 epsilon: 0.0500\n",
            "score: 160.0\n",
            "starting game  22 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  23 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  24 epsilon: 0.0500\n",
            "score: 285.0\n",
            "starting game  25 epsilon: 0.0500\n",
            "score: 285.0\n",
            "starting game  26 epsilon: 0.0500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uMquvpd5Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}